{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1db2c47e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T21:32:10.866653Z",
     "iopub.status.busy": "2026-02-20T21:32:10.866230Z",
     "iopub.status.idle": "2026-02-20T22:25:23.797689Z",
     "shell.execute_reply": "2026-02-20T22:25:23.796475Z"
    },
    "papermill": {
     "duration": 3192.938657,
     "end_time": "2026-02-20T22:25:23.799580",
     "exception": false,
     "start_time": "2026-02-20T21:32:10.860923",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-20 21:32:22.290945: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1771623142.580606      13 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1771623142.657938      13 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VESUVIUS V27 - OPTIMIZED HYSTERESIS\n",
      "============================================================\n",
      "\n",
      "Loading models...\n",
      "Loading Model 1 (ComboLoss): /kaggle/input/vsd-model/keras/transunet/3/transunet.seresnext50.160px.comboloss.weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-20 21:32:42.827965: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Inference...\n",
      "\n",
      "[1/1] Processing 1407735...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total patch 27:   0%|          | 0/14 [00:00<?, ?it/s]WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1771623185.777676      54 service.cc:148] XLA service 0x7f48dc018c60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1771623185.778685      54 service.cc:156]   StreamExecutor device (0): Host, Default Version\n",
      "I0000 00:00:1771623185.874815      54 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "Total patch 27: 100%|██████████| 14/14 [13:10<00:00, 56.47s/it]\n",
      "Total patch 27: 100%|██████████| 14/14 [13:02<00:00, 55.92s/it]\n",
      "Total patch 27: 100%|██████████| 14/14 [13:03<00:00, 55.97s/it]\n",
      "Total patch 27: 100%|██████████| 14/14 [13:05<00:00, 56.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Foreground voxels: 3,208,297\n",
      "\n",
      "============================================================\n",
      "V27 COMPLETE\n",
      "Submission: /kaggle/working/submission.zip\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# %% [code]\n",
    "\"\"\"\n",
    "================================================================================\n",
    "   VESUVIUS V27 - TUNED HYSTERESIS ENSEMBLE\n",
    "\n",
    "   - Base: V24 (Best Score: 0.537)\n",
    "   - Optimization 1: Ensemble Weights adjusted to 0.75/0.25 (Favoring ComboLoss)\n",
    "   - Optimization 2: Z_RADIUS increased to 2 (Better vertical connectivity)\n",
    "   - Optimization 3: T_HIGH lowered to 0.80 (Improved Recall)\n",
    "   - Constraint: Kept Overlap 0.25 & 4x TTA to ensure <9h runtime\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# ============================================================================\n",
    "# 1. SETUP PACKAGES\n",
    "# ============================================================================\n",
    "var = \"/kaggle/input/vsdetection-packages-offline-installer-only/whls\"\n",
    "if os.path.exists(var):\n",
    "    print(f\"Installing packages from: {var}\")\n",
    "    subprocess.run(\n",
    "        f\"pip install {var}/keras_nightly-*.whl {var}/tifffile-*.whl {var}/imagecodecs-*.whl {var}/medicai-*.whl --no-index --find-links {var}\",\n",
    "        shell=True, check=True\n",
    "    )\n",
    "    clear_output()\n",
    "else:\n",
    "    print(f\"❌ ERROR: Package directory not found at: {var}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "# Protobuf patch\n",
    "try:\n",
    "    from google.protobuf import message_factory as _message_factory\n",
    "    if not hasattr(_message_factory.MessageFactory, \"GetPrototype\"):\n",
    "        from google.protobuf.message_factory import GetMessageClass\n",
    "        def _GetPrototype(self, descriptor):\n",
    "            return GetMessageClass(descriptor)\n",
    "        _message_factory.MessageFactory.GetPrototype = _GetPrototype\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import keras\n",
    "from keras import ops\n",
    "from medicai.transforms import Compose, NormalizeIntensity \n",
    "from medicai.models import TransUNet\n",
    "from medicai.utils.inference import SlidingWindowInference\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import tifffile\n",
    "import scipy.ndimage as ndi\n",
    "from skimage.morphology import remove_small_objects\n",
    "import gc\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VESUVIUS V27 - OPTIMIZED HYSTERESIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIG\n",
    "# ============================================================================\n",
    "root_dir = \"/kaggle/input/vesuvius-challenge-surface-detection\"\n",
    "test_dir = f\"{root_dir}/test_images\"\n",
    "output_dir = \"/kaggle/working/submission_masks\"\n",
    "zip_path = \"/kaggle/working/submission.zip\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Model config\n",
    "NUM_CLASSES = 3\n",
    "PATCH_SIZE = (160, 160, 160)\n",
    "OVERLAP = 0.25      # Sharp inference\n",
    "BATCH_SIZE = 2      # Speed optimization\n",
    "\n",
    "# Post-processing config (Tuned V24)\n",
    "T_LOW = 0.45        # Keep connectivity permissive\n",
    "T_HIGH = 0.80       # Lowered from 0.85 to find more seeds\n",
    "Z_RADIUS = 2        # Increased from 1 to 2 (Fixes vertical splits)\n",
    "XY_RADIUS = 0       # Keep 0 (Prevent mergers)\n",
    "DUST_MIN_SIZE = 300 # Lowered from 500 to keep smaller valid fragments\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL LOADING\n",
    "# ============================================================================\n",
    "def get_ensemble_models():\n",
    "    base_path = \"/kaggle/input/vsd-model/keras/transunet\"\n",
    "    \n",
    "    # 1. The \"Super Model\" (V3 ComboLoss)\n",
    "    path_combo = f\"{base_path}/3/transunet.seresnext50.160px.comboloss.weights.h5\"\n",
    "    # 2. The \"Support Model\" (V2 Default) - Actually the TPU model path from V24\n",
    "    # Let's use the path we used in V24 which worked:\n",
    "    path_tpu = \"/kaggle/input/train-vesuvius-surface-3d-detection-on-tpu/model.weights.h5\"\n",
    "    \n",
    "    models = []\n",
    "    \n",
    "    if os.path.exists(path_combo):\n",
    "        print(f\"Loading Model 1 (ComboLoss): {path_combo}\")\n",
    "        m = TransUNet(input_shape=(160, 160, 160, 1), encoder_name='seresnext50', classifier_activation=None, num_classes=NUM_CLASSES)\n",
    "        m.load_weights(path_combo)\n",
    "        models.append(m)\n",
    "        \n",
    "    if os.path.exists(path_tpu):\n",
    "        print(f\"Loading Model 2 (TPU): {path_tpu}\")\n",
    "        m = TransUNet(input_shape=(160, 160, 160, 1), encoder_name='seresnext50', classifier_activation=None, num_classes=NUM_CLASSES)\n",
    "        m.load_weights(path_tpu)\n",
    "        models.append(m)\n",
    "        \n",
    "    if not models:\n",
    "        print(\"WARNING: No weights found. Initializing random model.\")\n",
    "        m = TransUNet(input_shape=(160, 160, 160, 1), encoder_name='seresnext50', classifier_activation=None, num_classes=NUM_CLASSES)\n",
    "        return [m]\n",
    "        \n",
    "    return models\n",
    "\n",
    "# ============================================================================\n",
    "# TRANSFORMS\n",
    "# ============================================================================\n",
    "def val_transformation(image):\n",
    "    data = {\"image\": image}\n",
    "    pipeline = Compose([\n",
    "        NormalizeIntensity(keys=[\"image\"], nonzero=True, channel_wise=False)\n",
    "    ])\n",
    "    result = pipeline(data)\n",
    "    return result[\"image\"]\n",
    "\n",
    "def load_volume(path):\n",
    "    vol = tifffile.imread(path).astype(np.float32)\n",
    "    vol = vol[None, ..., None]\n",
    "    return vol\n",
    "\n",
    "# ============================================================================\n",
    "# INFERENCE LOGIC (4x TTA)\n",
    "# ============================================================================\n",
    "def predict_with_tta(inputs, swi):\n",
    "    logits = []\n",
    "    # Original\n",
    "    logits.append(swi(inputs))\n",
    "    # Rotations\n",
    "    for k in [1, 2, 3]:\n",
    "        img_r = np.rot90(inputs, k=k, axes=(2, 3))\n",
    "        p = swi(img_r)\n",
    "        p = np.rot90(p, k=-k, axes=(2, 3))\n",
    "        logits.append(p)\n",
    "    return np.mean(logits, axis=0)\n",
    "\n",
    "def ensemble_predict(inputs, models):\n",
    "    # Weighted Ensemble (Aligned with 0.546 solution)\n",
    "    if len(models) == 2:\n",
    "        weights = [0.75, 0.25] # Stronger weight for ComboLoss\n",
    "    else:\n",
    "        weights = [1.0]\n",
    "        \n",
    "    ensemble_probs = []\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        swi = SlidingWindowInference(\n",
    "            model, \n",
    "            num_classes=NUM_CLASSES, \n",
    "            roi_size=PATCH_SIZE, \n",
    "            sw_batch_size=BATCH_SIZE, \n",
    "            mode='gaussian', \n",
    "            overlap=OVERLAP\n",
    "        )\n",
    "        \n",
    "        logits = predict_with_tta(inputs, swi)\n",
    "        probs = ops.softmax(logits, axis=-1)\n",
    "        fg_probs = probs[..., 1] \n",
    "        ensemble_probs.append(fg_probs * weights[i])\n",
    "    \n",
    "    final_probs = np.sum(ensemble_probs, axis=0)\n",
    "    return np.squeeze(final_probs)\n",
    "\n",
    "# ============================================================================\n",
    "# POST-PROCESSING (HYSTERESIS)\n",
    "# ============================================================================\n",
    "def build_anisotropic_struct(z_radius, xy_radius):\n",
    "    z, r = z_radius, xy_radius\n",
    "    if z == 0 and r == 0: return None\n",
    "    depth = 2 * z + 1\n",
    "    size = 2 * r + 1\n",
    "    struct = np.zeros((depth, size, size), dtype=bool)\n",
    "    cz, cy, cx = z, r, r\n",
    "    for dz in range(-z, z + 1):\n",
    "        for dy in range(-r, r + 1):\n",
    "            for dx in range(-r, r + 1):\n",
    "                if dy * dy + dx * dx <= r * r:\n",
    "                    struct[cz + dz, cy + dy, cx + dx] = True\n",
    "    return struct\n",
    "\n",
    "def topo_postprocess(probs, T_low, T_high, z_radius, xy_radius, dust_min_size):\n",
    "    # 1. Hysteresis\n",
    "    strong = probs >= T_high\n",
    "    weak = probs >= T_low\n",
    "    \n",
    "    if not strong.any(): return np.zeros_like(probs, dtype=np.uint8)\n",
    "    \n",
    "    struct_hyst = ndi.generate_binary_structure(3, 3)\n",
    "    mask = ndi.binary_propagation(strong, mask=weak, structure=struct_hyst)\n",
    "    \n",
    "    if not mask.any(): return np.zeros_like(probs, dtype=np.uint8)\n",
    "\n",
    "    # 2. Anisotropic Closing\n",
    "    struct = build_anisotropic_struct(z_radius, xy_radius)\n",
    "    if struct is not None:\n",
    "        mask = ndi.binary_closing(mask, structure=struct)\n",
    "\n",
    "    # 3. Dust Removal\n",
    "    if dust_min_size > 0:\n",
    "        mask = remove_small_objects(mask.astype(bool), min_size=dust_min_size)\n",
    "\n",
    "    return mask.astype(np.uint8)\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN PIPELINE\n",
    "# ============================================================================\n",
    "print(\"\\nLoading models...\")\n",
    "models = get_ensemble_models()\n",
    "test_df = pd.read_csv(f\"{root_dir}/test.csv\")\n",
    "\n",
    "print(\"\\nStarting Inference...\")\n",
    "with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
    "    for idx, row in test_df.iterrows():\n",
    "        image_id = row[\"id\"]\n",
    "        print(f\"\\n[{idx+1}/{len(test_df)}] Processing {image_id}...\")\n",
    "        \n",
    "        vol = load_volume(f\"{test_dir}/{image_id}.tif\")\n",
    "        vol = val_transformation(vol)\n",
    "        \n",
    "        probs = ensemble_predict(vol, models)\n",
    "        \n",
    "        final_mask = topo_postprocess(\n",
    "            probs, \n",
    "            T_low=T_LOW, \n",
    "            T_high=T_HIGH, \n",
    "            z_radius=Z_RADIUS, \n",
    "            xy_radius=XY_RADIUS, \n",
    "            dust_min_size=DUST_MIN_SIZE\n",
    "        )\n",
    "        \n",
    "        print(f\"    Foreground voxels: {final_mask.sum():,}\")\n",
    "        \n",
    "        out_path = f\"{output_dir}/{image_id}.tif\"\n",
    "        tifffile.imwrite(out_path, final_mask)\n",
    "        z.write(out_path, arcname=f\"{image_id}.tif\")\n",
    "        os.remove(out_path)\n",
    "        \n",
    "        del vol, probs, final_mask\n",
    "        gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"V27 COMPLETE\")\n",
    "print(f\"Submission: {zip_path}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15062069,
     "sourceId": 117682,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 15477237,
     "modelId": 510647,
     "modelInstanceId": 516822,
     "sourceId": 732880,
     "sourceType": "modelInstanceVersion"
    },
    {
     "databundleVersionId": 14910215,
     "modelId": 510647,
     "modelInstanceId": 516822,
     "sourceId": 681152,
     "sourceType": "modelInstanceVersion"
    },
    {
     "databundleVersionId": 14570138,
     "modelId": 510647,
     "modelInstanceId": 495238,
     "sourceId": 655294,
     "sourceType": "modelInstanceVersion"
    },
    {
     "databundleVersionId": 14687610,
     "modelId": 510647,
     "modelInstanceId": 503784,
     "sourceId": 665589,
     "sourceType": "modelInstanceVersion"
    },
    {
     "databundleVersionId": 14761443,
     "modelId": 510647,
     "modelInstanceId": 495238,
     "sourceId": 672178,
     "sourceType": "modelInstanceVersion"
    },
    {
     "databundleVersionId": 14811492,
     "modelId": 510647,
     "modelInstanceId": 503784,
     "sourceId": 674747,
     "sourceType": "modelInstanceVersion"
    },
    {
     "databundleVersionId": 14691066,
     "modelId": 510647,
     "modelInstanceId": 504051,
     "sourceId": 665924,
     "sourceType": "modelInstanceVersion"
    },
    {
     "databundleVersionId": 14789938,
     "modelId": 510647,
     "modelInstanceId": 499479,
     "sourceId": 673516,
     "sourceType": "modelInstanceVersion"
    },
    {
     "databundleVersionId": 14626934,
     "modelId": 510647,
     "modelInstanceId": 499479,
     "sourceId": 660383,
     "sourceType": "modelInstanceVersion"
    },
    {
     "sourceId": 290917305,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3200.939188,
   "end_time": "2026-02-20T22:25:26.629826",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-20T21:32:05.690638",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
