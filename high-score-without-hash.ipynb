{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8617b16e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T19:38:36.011993Z",
     "iopub.status.busy": "2026-02-16T19:38:36.011656Z",
     "iopub.status.idle": "2026-02-16T19:38:36.016065Z",
     "shell.execute_reply": "2026-02-16T19:38:36.015433Z",
     "shell.execute_reply.started": "2026-02-16T19:38:36.011965Z"
    },
    "papermill": {
     "duration": 0.002929,
     "end_time": "2026-02-22T09:11:48.429450",
     "exception": false,
     "start_time": "2026-02-22T09:11:48.426521",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Reference\n",
    "\n",
    "https://www.kaggle.com/code/llkh0a/stanford-rna-3d-folding-part-2-protenix-tbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fecce49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T09:11:48.434817Z",
     "iopub.status.busy": "2026-02-22T09:11:48.434555Z",
     "iopub.status.idle": "2026-02-22T09:11:48.438124Z",
     "shell.execute_reply": "2026-02-22T09:11:48.437641Z"
    },
    "papermill": {
     "duration": 0.007818,
     "end_time": "2026-02-22T09:11:48.439429",
     "exception": false,
     "start_time": "2026-02-22T09:11:48.431611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install /kaggle/input/datasets/ogurtsov/biopython/biopython-1.85-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ae04c66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T09:11:48.444931Z",
     "iopub.status.busy": "2026-02-22T09:11:48.444281Z",
     "iopub.status.idle": "2026-02-22T09:11:49.182270Z",
     "shell.execute_reply": "2026-02-22T09:11:49.181482Z"
    },
    "papermill": {
     "duration": 0.742379,
     "end_time": "2026-02-22T09:11:49.183867",
     "exception": false,
     "start_time": "2026-02-22T09:11:48.441488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in LOCAL mode — only the first 2 test targets will be processed to save time.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# ── Local vs Kaggle mode ─────────────────────────────────────────────────────\n",
    "# On Kaggle competition rerun, KAGGLE_IS_COMPETITION_RERUN is set to a truthy value.\n",
    "# When running locally we do NOT exit — instead we cap the test set to a small\n",
    "# number of samples so the notebook finishes quickly.\n",
    "\n",
    "IS_KAGGLE = bool(os.environ.get(\"KAGGLE_IS_COMPETITION_RERUN\", \"\"))\n",
    "\n",
    "# How many test samples to use when running locally\n",
    "LOCAL_N_SAMPLES = 2\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    print(\"Running in KAGGLE COMPETITION mode — all test targets will be processed.\")\n",
    "else:\n",
    "    print(f\"Running in LOCAL mode — only the first {LOCAL_N_SAMPLES} test targets \"\n",
    "          f\"will be processed to save time.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0edd50a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T09:11:49.189714Z",
     "iopub.status.busy": "2026-02-22T09:11:49.189355Z",
     "iopub.status.idle": "2026-02-22T09:11:52.678858Z",
     "shell.execute_reply": "2026-02-22T09:11:52.678079Z"
    },
    "papermill": {
     "duration": 3.494291,
     "end_time": "2026-02-22T09:11:52.680533",
     "exception": false,
     "start_time": "2026-02-22T09:11:49.186242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "os.environ[\"LAYERNORM_TYPE\"] = \"torch\"\n",
    "os.environ.setdefault(\"RNA_MSA_DEPTH_LIMIT\", \"512\")\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from Bio.Align import PairwiseAligner\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7bc6a6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T09:11:52.686443Z",
     "iopub.status.busy": "2026-02-22T09:11:52.686089Z",
     "iopub.status.idle": "2026-02-22T09:11:52.692687Z",
     "shell.execute_reply": "2026-02-22T09:11:52.692110Z"
    },
    "papermill": {
     "duration": 0.011177,
     "end_time": "2026-02-22T09:11:52.694079",
     "exception": false,
     "start_time": "2026-02-22T09:11:52.682902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_c1_mask(data: dict, atom_array) -> torch.Tensor:\n",
    "    # 1. Try atom_array attributes first\n",
    "    if atom_array is not None:\n",
    "        try:\n",
    "            if hasattr(atom_array, \"centre_atom_mask\"):\n",
    "                m = atom_array.centre_atom_mask == 1\n",
    "                if hasattr(atom_array, \"is_rna\"):\n",
    "                    m = m & atom_array.is_rna\n",
    "                return torch.from_numpy(m).bool()\n",
    "            \n",
    "            if hasattr(atom_array, \"atom_name\"):\n",
    "                base = atom_array.atom_name == \"C1'\"\n",
    "                if hasattr(atom_array, \"is_rna\"):\n",
    "                    base = base & atom_array.is_rna\n",
    "                return torch.from_numpy(base).bool()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 2. Fallback to feature dict\n",
    "    f = data[\"input_feature_dict\"]\n",
    "    \n",
    "    if \"centre_atom_mask\" in f:\n",
    "        return (f[\"centre_atom_mask\"] == 1).bool()\n",
    "    if \"center_atom_mask\" in f:\n",
    "        return (f[\"center_atom_mask\"] == 1).bool()\n",
    "        \n",
    "    # Heuristic fallback: check which index gives us roughly N_token atoms\n",
    "    n_tokens = data.get(\"N_token\", torch.tensor(0)).item()\n",
    "    mask11 = (f[\"atom_to_tokatom_idx\"] == 11).bool()\n",
    "    mask12 = (f[\"atom_to_tokatom_idx\"] == 12).bool()\n",
    "    \n",
    "    c11 = mask11.sum().item()\n",
    "    c12 = mask12.sum().item()\n",
    "    \n",
    "    # Return the one closer to N_tokens (likely one per residue)\n",
    "    if abs(c11 - n_tokens) < abs(c12 - n_tokens):\n",
    "        return mask11\n",
    "    else:\n",
    "        return mask12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23c59b38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T09:11:52.700459Z",
     "iopub.status.busy": "2026-02-22T09:11:52.700219Z",
     "iopub.status.idle": "2026-02-22T09:11:52.778441Z",
     "shell.execute_reply": "2026-02-22T09:11:52.777747Z"
    },
    "papermill": {
     "duration": 0.083629,
     "end_time": "2026-02-22T09:11:52.779880",
     "exception": false,
     "start_time": "2026-02-22T09:11:52.696251",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/Bio/Align/__init__.py:4414: BiopythonDeprecationWarning: The attribute 'query_left_open_gap_score' was renamed to 'open_left_deletion_score'. This was done to be consistent with the\n",
      "AlignmentCounts object returned by the .counts method of an Alignment object.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/Bio/Align/__init__.py:4414: BiopythonDeprecationWarning: The attribute 'query_left_extend_gap_score' was renamed to 'extend_left_deletion_score'. This was done to be consistent with the\n",
      "AlignmentCounts object returned by the .counts method of an Alignment object.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/Bio/Align/__init__.py:4414: BiopythonDeprecationWarning: The attribute 'query_right_open_gap_score' was renamed to 'open_right_deletion_score'. This was done to be consistent with the\n",
      "AlignmentCounts object returned by the .counts method of an Alignment object.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/Bio/Align/__init__.py:4414: BiopythonDeprecationWarning: The attribute 'query_right_extend_gap_score' was renamed to 'extend_right_deletion_score'. This was done to be consistent with the\n",
      "AlignmentCounts object returned by the .counts method of an Alignment object.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/Bio/Align/__init__.py:4414: BiopythonDeprecationWarning: The attribute 'target_left_open_gap_score' was renamed to 'open_left_insertion_score'. This was done to be consistent with the\n",
      "AlignmentCounts object returned by the .counts method of an Alignment object.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/Bio/Align/__init__.py:4414: BiopythonDeprecationWarning: The attribute 'target_left_extend_gap_score' was renamed to 'extend_left_insertion_score'. This was done to be consistent with the\n",
      "AlignmentCounts object returned by the .counts method of an Alignment object.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/Bio/Align/__init__.py:4414: BiopythonDeprecationWarning: The attribute 'target_right_open_gap_score' was renamed to 'open_right_insertion_score'. This was done to be consistent with the\n",
      "AlignmentCounts object returned by the .counts method of an Alignment object.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/Bio/Align/__init__.py:4414: BiopythonDeprecationWarning: The attribute 'target_right_extend_gap_score' was renamed to 'extend_right_insertion_score'. This was done to be consistent with the\n",
      "AlignmentCounts object returned by the .counts method of an Alignment object.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ─────────────── Paths & Constants ───────────────────────────────────────────\n",
    "DATA_BASE              = \"/kaggle/input/stanford-rna-3d-folding-2\"\n",
    "DEFAULT_TEST_CSV       = f\"{DATA_BASE}/test_sequences.csv\"\n",
    "DEFAULT_TRAIN_CSV      = f\"{DATA_BASE}/train_sequences.csv\"\n",
    "DEFAULT_TRAIN_LBLS     = f\"{DATA_BASE}/train_labels.csv\"\n",
    "DEFAULT_VAL_CSV        = f\"{DATA_BASE}/validation_sequences.csv\"\n",
    "DEFAULT_VAL_LBLS       = f\"{DATA_BASE}/validation_labels.csv\"\n",
    "DEFAULT_OUTPUT         = \"/kaggle/working/submission.csv\"\n",
    "\n",
    "DEFAULT_CODE_DIR = (\n",
    "    \"/kaggle/input/datasets/qiweiyin/protenix-v1-adjusted\"\n",
    "    \"/Protenix-v1-adjust-v2/Protenix-v1-adjust-v2/Protenix-v1\"\n",
    ")\n",
    "DEFAULT_ROOT_DIR = DEFAULT_CODE_DIR\n",
    "\n",
    "MODEL_NAME    = \"protenix_base_20250630_v1.0.0\"\n",
    "N_SAMPLE      = 5\n",
    "SEED          = 42\n",
    "MAX_SEQ_LEN   = int(os.environ.get(\"MAX_SEQ_LEN\",   \"512\"))\n",
    "CHUNK_OVERLAP = int(os.environ.get(\"CHUNK_OVERLAP\",  \"128\"))\n",
    "\n",
    "# TBM quality thresholds — sequences below these get routed to Protenix\n",
    "MIN_SIMILARITY       = float(os.environ.get(\"MIN_SIMILARITY\",       \"0.0\"))\n",
    "MIN_PERCENT_IDENTITY = float(os.environ.get(\"MIN_PERCENT_IDENTITY\", \"50.0\"))\n",
    "\n",
    "# Set False to skip Protenix and use de-novo fallback instead\n",
    "USE_PROTENIX = True\n",
    "\n",
    "\n",
    "def parse_bool(value: str, default: bool = False) -> str:\n",
    "    v = str(value).strip().lower()\n",
    "    if v in {\"1\", \"true\", \"t\", \"yes\", \"y\", \"on\"}:\n",
    "        return \"true\"\n",
    "    if v in {\"0\", \"false\", \"f\", \"no\", \"n\", \"off\"}:\n",
    "        return \"false\"\n",
    "    return \"true\" if default else \"false\"\n",
    "\n",
    "\n",
    "USE_MSA      = parse_bool(os.environ.get(\"USE_MSA\",      \"false\"))\n",
    "USE_TEMPLATE = parse_bool(os.environ.get(\"USE_TEMPLATE\", \"false\"))\n",
    "USE_RNA_MSA  = parse_bool(os.environ.get(\"USE_RNA_MSA\",  \"true\"))\n",
    "\n",
    "MODEL_N_SAMPLE = int(os.environ.get(\"MODEL_N_SAMPLE\", str(N_SAMPLE)))\n",
    "\n",
    "\n",
    "# ─────────────── General Utilities ───────────────────────────────────────────\n",
    "def seed_everything(seed: int) -> None:\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "def resolve_paths():\n",
    "    test_csv   = os.environ.get(\"TEST_CSV\",           DEFAULT_TEST_CSV)\n",
    "    output_csv = os.environ.get(\"SUBMISSION_CSV\",     DEFAULT_OUTPUT)\n",
    "    code_dir   = os.environ.get(\"PROTENIX_CODE_DIR\",  DEFAULT_CODE_DIR)\n",
    "    root_dir   = os.environ.get(\"PROTENIX_ROOT_DIR\",  DEFAULT_ROOT_DIR)\n",
    "    return test_csv, output_csv, code_dir, root_dir\n",
    "\n",
    "\n",
    "def ensure_required_files(root_dir: str) -> None:\n",
    "    for p, name in [\n",
    "        (Path(root_dir) / \"checkpoint\" / f\"{MODEL_NAME}.pt\",          \"checkpoint\"),\n",
    "        (Path(root_dir) / \"common\" / \"components.cif\",                \"CCD file\"),\n",
    "        (Path(root_dir) / \"common\" / \"components.cif.rdkit_mol.pkl\",  \"CCD cache\"),\n",
    "    ]:\n",
    "        if not p.exists():\n",
    "            raise FileNotFoundError(f\"Missing {name}: {p}\")\n",
    "\n",
    "\n",
    "# ─────────────── Protenix Input / Config Helpers ─────────────────────────────\n",
    "def build_input_json(df: pd.DataFrame, json_path: str) -> None:\n",
    "    data = [\n",
    "        {\n",
    "            \"name\": row[\"target_id\"],\n",
    "            \"covalent_bonds\": [],\n",
    "            \"sequences\": [{\"rnaSequence\": {\"sequence\": row[\"sequence\"], \"count\": 1}}],\n",
    "        }\n",
    "        for _, row in df.iterrows()\n",
    "    ]\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "\n",
    "def build_configs(input_json_path: str, dump_dir: str, model_name: str):\n",
    "    from configs.configs_base import configs as configs_base\n",
    "    from configs.configs_data import data_configs\n",
    "    from configs.configs_inference import inference_configs\n",
    "    from configs.configs_model_type import model_configs\n",
    "    from protenix.config.config import parse_configs\n",
    "\n",
    "    base = {**configs_base, **{\"data\": data_configs}, **inference_configs}\n",
    "\n",
    "    def deep_update(t, p):\n",
    "        for k, v in p.items():\n",
    "            if isinstance(v, dict) and k in t and isinstance(t[k], dict):\n",
    "                deep_update(t[k], v)\n",
    "            else:\n",
    "                t[k] = v\n",
    "\n",
    "    deep_update(base, model_configs[model_name])\n",
    "    arg_str = \" \".join([\n",
    "        f\"--model_name {model_name}\",\n",
    "        f\"--input_json_path {input_json_path}\",\n",
    "        f\"--dump_dir {dump_dir}\",\n",
    "        f\"--use_msa {USE_MSA}\",\n",
    "        f\"--use_template {USE_TEMPLATE}\",\n",
    "        f\"--use_rna_msa {USE_RNA_MSA}\",\n",
    "        f\"--sample_diffusion.N_sample {MODEL_N_SAMPLE}\",\n",
    "        f\"--seeds {SEED}\",\n",
    "    ])\n",
    "    return parse_configs(configs=base, arg_str=arg_str, fill_required_with_null=True)\n",
    "\n",
    "\n",
    "def get_c1_mask(data: dict, atom_array) -> torch.Tensor:\n",
    "    # 1. Try atom_array attributes first\n",
    "    if atom_array is not None:\n",
    "        try:\n",
    "            if hasattr(atom_array, \"centre_atom_mask\"):\n",
    "                m = atom_array.centre_atom_mask == 1\n",
    "                if hasattr(atom_array, \"is_rna\"):\n",
    "                    m = m & atom_array.is_rna\n",
    "                return torch.from_numpy(m).bool()\n",
    "            \n",
    "            if hasattr(atom_array, \"atom_name\"):\n",
    "                base = atom_array.atom_name == \"C1'\"\n",
    "                if hasattr(atom_array, \"is_rna\"):\n",
    "                    base = base & atom_array.is_rna\n",
    "                return torch.from_numpy(base).bool()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 2. Fallback to feature dict\n",
    "    f = data[\"input_feature_dict\"]\n",
    "    \n",
    "    # CASE A: center_atom_mask exists\n",
    "    if \"center_atom_mask\" in f:\n",
    "        return (f[\"center_atom_mask\"] == 1).bool()\n",
    "    if \"centre_atom_mask\" in f:\n",
    "        return (f[\"centre_atom_mask\"] == 1).bool()\n",
    "        \n",
    "    # CASE B: Use atom_name\n",
    "    if \"atom_name\" in f:\n",
    "        # Check against \"C1'\" (byte encoded or string?)\n",
    "        # For now assume typical behavior is center_atom_mask is present.\n",
    "        pass\n",
    "\n",
    "    # CASE C: atom_to_tokatom_idx fallback\n",
    "    # The index for C1' is typically 11 or 12 depending on featurizer.\n",
    "    # Let's try to match exactly C1' if possible.\n",
    "    # But usually 'centre_atom_mask' should be there.\n",
    "    \n",
    "    # If we fall through, assume standard mask\n",
    "    return (f[\"atom_to_tokatom_idx\"] == 11).bool()\n",
    "\n",
    "\n",
    "def get_feature_c1_mask(data: dict) -> torch.Tensor:\n",
    "    f = data[\"input_feature_dict\"]\n",
    "    if \"centre_atom_mask\" in f:\n",
    "        return f[\"centre_atom_mask\"].long() == 1\n",
    "    return f[\"atom_to_tokatom_idx\"].long() == 12\n",
    "\n",
    "\n",
    "def coords_to_rows(target_id: str, seq: str, coords: np.ndarray) -> list:\n",
    "    \"\"\"coords shape: (N_SAMPLE, seq_len, 3)\"\"\"\n",
    "    rows = []\n",
    "    for i in range(len(seq)):\n",
    "        row = {\"ID\": f\"{target_id}_{i + 1}\", \"resname\": seq[i], \"resid\": i + 1}\n",
    "        for s in range(N_SAMPLE):\n",
    "            if s < coords.shape[0] and i < coords.shape[1]:\n",
    "                x, y, z = coords[s, i]\n",
    "            else:\n",
    "                x, y, z = 0.0, 0.0, 0.0\n",
    "            row[f\"x_{s + 1}\"] = float(x)\n",
    "            row[f\"y_{s + 1}\"] = float(y)\n",
    "            row[f\"z_{s + 1}\"] = float(z)\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "\n",
    "def pad_samples(coords: np.ndarray, n: int) -> np.ndarray:\n",
    "    if coords.shape[0] >= n:\n",
    "        return coords[:n]\n",
    "    if coords.shape[0] == 0:\n",
    "        return np.zeros((n, coords.shape[1], 3), dtype=coords.dtype)\n",
    "    extra = np.repeat(coords[:1], n - coords.shape[0], axis=0)\n",
    "    return np.concatenate([coords, extra], axis=0)\n",
    "\n",
    "\n",
    "# ─────────────── TBM Core Functions ──────────────────────────────────────────\n",
    "def _make_aligner() -> PairwiseAligner:\n",
    "    al = PairwiseAligner()\n",
    "    al.mode                           = \"global\"\n",
    "    al.match_score                    = 2\n",
    "    al.mismatch_score                 = -1.5\n",
    "    al.open_gap_score                 = -8\n",
    "    al.extend_gap_score               = -0.4\n",
    "    al.query_left_open_gap_score      = -8\n",
    "    al.query_left_extend_gap_score    = -0.4\n",
    "    al.query_right_open_gap_score     = -8\n",
    "    al.query_right_extend_gap_score   = -0.4\n",
    "    al.target_left_open_gap_score     = -8\n",
    "    al.target_left_extend_gap_score   = -0.4\n",
    "    al.target_right_open_gap_score    = -8\n",
    "    al.target_right_extend_gap_score  = -0.4\n",
    "    return al\n",
    "\n",
    "\n",
    "_aligner = _make_aligner()\n",
    "\n",
    "\n",
    "def parse_stoichiometry(stoich: str) -> list:\n",
    "    if pd.isna(stoich) or str(stoich).strip() == \"\":\n",
    "        return []\n",
    "    return [(ch.strip(), int(cnt)) for part in str(stoich).split(\";\")\n",
    "            for ch, cnt in [part.split(\":\")]]\n",
    "\n",
    "\n",
    "def parse_fasta(fasta_content: str) -> dict:\n",
    "    out, cur, parts = {}, None, []\n",
    "    for line in str(fasta_content).splitlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if line.startswith(\">\"):\n",
    "            if cur is not None:\n",
    "                out[cur] = \"\".join(parts)\n",
    "            cur = line[1:].split()[0]\n",
    "            parts = []\n",
    "        else:\n",
    "            parts.append(line.replace(\" \", \"\"))\n",
    "    if cur is not None:\n",
    "        out[cur] = \"\".join(parts)\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_chain_segments(row) -> list:\n",
    "    seq    = row[\"sequence\"]\n",
    "    stoich = row.get(\"stoichiometry\", \"\")\n",
    "    all_sq = row.get(\"all_sequences\", \"\")\n",
    "    if (pd.isna(stoich) or pd.isna(all_sq)\n",
    "            or str(stoich).strip() == \"\" or str(all_sq).strip() == \"\"):\n",
    "        return [(0, len(seq))]\n",
    "    try:\n",
    "        chain_dict = parse_fasta(all_sq)\n",
    "        order = parse_stoichiometry(stoich)\n",
    "        segs, pos = [], 0\n",
    "        for ch, cnt in order:\n",
    "            base = chain_dict.get(ch)\n",
    "            if base is None:\n",
    "                return [(0, len(seq))]\n",
    "            for _ in range(cnt):\n",
    "                segs.append((pos, pos + len(base)))\n",
    "                pos += len(base)\n",
    "        return segs if pos == len(seq) else [(0, len(seq))]\n",
    "    except Exception:\n",
    "        return [(0, len(seq))]\n",
    "\n",
    "\n",
    "def build_segments_map(df: pd.DataFrame) -> tuple:\n",
    "    seg_map, stoich_map = {}, {}\n",
    "    for _, r in df.iterrows():\n",
    "        tid               = r[\"target_id\"]\n",
    "        seg_map[tid]      = get_chain_segments(r)\n",
    "        raw_s             = r.get(\"stoichiometry\", \"\")\n",
    "        stoich_map[tid]   = \"\" if pd.isna(raw_s) else str(raw_s)\n",
    "    return seg_map, stoich_map\n",
    "\n",
    "\n",
    "def process_labels(labels_df: pd.DataFrame) -> dict:\n",
    "    coords = {}\n",
    "    prefixes = labels_df[\"ID\"].str.rsplit(\"_\", n=1).str[0]\n",
    "    for prefix, grp in labels_df.groupby(prefixes):\n",
    "        coords[prefix] = grp.sort_values(\"resid\")[[\"x_1\", \"y_1\", \"z_1\"]].values\n",
    "    return coords\n",
    "\n",
    "\n",
    "def _build_aligned_strings(query_seq, template_seq, alignment):\n",
    "    q_segs, t_segs = alignment.aligned\n",
    "    aq, at, qi, ti = [], [], 0, 0\n",
    "    for (qs, qe), (ts, te) in zip(q_segs, t_segs):\n",
    "        while qi < qs: aq.append(query_seq[qi]);    at.append(\"-\");              qi += 1\n",
    "        while ti < ts: aq.append(\"-\");              at.append(template_seq[ti]); ti += 1\n",
    "        for qp, tp in zip(range(qs, qe), range(ts, te)):\n",
    "            aq.append(query_seq[qp]); at.append(template_seq[tp])\n",
    "        qi, ti = qe, te\n",
    "    while qi < len(query_seq):    aq.append(query_seq[qi]);    at.append(\"-\");              qi += 1\n",
    "    while ti < len(template_seq): aq.append(\"-\");              at.append(template_seq[ti]); ti += 1\n",
    "    return \"\".join(aq), \"\".join(at)\n",
    "\n",
    "\n",
    "def find_similar_sequences_detailed(query_seq, train_seqs_df, train_coords_dict, top_n=30):\n",
    "    results = []\n",
    "    for _, row in train_seqs_df.iterrows():\n",
    "        tid, tseq = row[\"target_id\"], row[\"sequence\"]\n",
    "        if tid not in train_coords_dict:\n",
    "            continue\n",
    "        if abs(len(tseq) - len(query_seq)) / max(len(tseq), len(query_seq)) > 0.3:\n",
    "            continue\n",
    "        aln       = next(iter(_aligner.align(query_seq, tseq)))\n",
    "        norm_s    = aln.score / (2 * min(len(query_seq), len(tseq)))\n",
    "        identical = sum(\n",
    "            1 for (qs, qe), (ts, te) in zip(*aln.aligned)\n",
    "            for qp, tp in zip(range(qs, qe), range(ts, te))\n",
    "            if query_seq[qp] == tseq[tp]\n",
    "        )\n",
    "        pct_id = 100 * identical / len(query_seq)\n",
    "        aq, at = _build_aligned_strings(query_seq, tseq, aln)\n",
    "        results.append((tid, tseq, norm_s, train_coords_dict[tid], pct_id, aq, at))\n",
    "    results.sort(key=lambda x: x[2], reverse=True)\n",
    "    return results[:top_n]\n",
    "\n",
    "\n",
    "def adapt_template_to_query(query_seq, template_seq, template_coords) -> np.ndarray:\n",
    "    aln        = next(iter(_aligner.align(query_seq, template_seq)))\n",
    "    new_coords = np.full((len(query_seq), 3), np.nan)\n",
    "    for (qs, qe), (ts, te) in zip(*aln.aligned):\n",
    "        chunk = template_coords[ts:te]\n",
    "        if len(chunk) == (qe - qs):\n",
    "            new_coords[qs:qe] = chunk\n",
    "    for i in range(len(new_coords)):\n",
    "        if np.isnan(new_coords[i, 0]):\n",
    "            pv = next((j for j in range(i - 1, -1, -1) if not np.isnan(new_coords[j, 0])), -1)\n",
    "            nv = next((j for j in range(i + 1, len(new_coords)) if not np.isnan(new_coords[j, 0])), -1)\n",
    "            if pv >= 0 and nv >= 0:\n",
    "                w = (i - pv) / (nv - pv)\n",
    "                new_coords[i] = (1 - w) * new_coords[pv] + w * new_coords[nv]\n",
    "            elif pv >= 0:\n",
    "                new_coords[i] = new_coords[pv] + [3, 0, 0]\n",
    "            elif nv >= 0:\n",
    "                new_coords[i] = new_coords[nv] + [3, 0, 0]\n",
    "            else:\n",
    "                new_coords[i] = [i * 3, 0, 0]\n",
    "    return np.nan_to_num(new_coords)\n",
    "\n",
    "\n",
    "def adaptive_rna_constraints(coords, target_id, segments_map, confidence=1.0, passes=2) -> np.ndarray:\n",
    "    X        = coords.copy()\n",
    "    segments = segments_map.get(target_id, [(0, len(X))])\n",
    "    strength = max(0.75 * (1.0 - min(confidence, 0.97)), 0.02)\n",
    "    for _ in range(passes):\n",
    "        for s, e in segments:\n",
    "            C = X[s:e]; L = e - s\n",
    "            if L < 3:\n",
    "                continue\n",
    "            # bond i–i+1  ~5.95 Å\n",
    "            d    = C[1:] - C[:-1]; dist = np.linalg.norm(d, axis=1) + 1e-6\n",
    "            adj  = d * ((5.95 - dist) / dist)[:, None] * (0.22 * strength)\n",
    "            C[:-1] -= adj; C[1:] += adj\n",
    "            # soft i–i+2  ~10.2 Å\n",
    "            d2   = C[2:] - C[:-2]; d2n = np.linalg.norm(d2, axis=1) + 1e-6\n",
    "            adj2 = d2 * ((10.2 - d2n) / d2n)[:, None] * (0.10 * strength)\n",
    "            C[:-2] -= adj2; C[2:] += adj2\n",
    "            # Laplacian smoothing\n",
    "            C[1:-1] += (0.06 * strength) * (0.5 * (C[:-2] + C[2:]) - C[1:-1])\n",
    "            # self-avoidance\n",
    "            if L >= 25:\n",
    "                idx  = np.linspace(0, L - 1, min(L, 160)).astype(int) if L > 220 else np.arange(L)\n",
    "                P    = C[idx]; diff = P[:, None, :] - P[None, :, :]\n",
    "                dm   = np.linalg.norm(diff, axis=2) + 1e-6\n",
    "                sep  = np.abs(idx[:, None] - idx[None, :])\n",
    "                mask = (sep > 2) & (dm < 3.2)\n",
    "                if np.any(mask):\n",
    "                    vec = (diff * ((3.2 - dm) / dm)[:, :, None] * mask[:, :, None]).sum(axis=1)\n",
    "                    C[idx] += (0.015 * strength) * vec\n",
    "            X[s:e] = C\n",
    "    return X\n",
    "\n",
    "\n",
    "def _rotmat(axis, ang):\n",
    "    a = np.asarray(axis, float); a /= np.linalg.norm(a) + 1e-12\n",
    "    x, y, z = a; c, s = np.cos(ang), np.sin(ang); CC = 1 - c\n",
    "    return np.array([[c+x*x*CC, x*y*CC-z*s, x*z*CC+y*s],\n",
    "                     [y*x*CC+z*s, c+y*y*CC, y*z*CC-x*s],\n",
    "                     [z*x*CC-y*s, z*y*CC+x*s, c+z*z*CC]])\n",
    "\n",
    "\n",
    "def apply_hinge(coords, seg, rng, deg=22):\n",
    "    s, e = seg; L = e - s\n",
    "    if L < 30: return coords\n",
    "    pivot = s + int(rng.integers(10, L - 10))\n",
    "    R = _rotmat(rng.normal(size=3), np.deg2rad(float(rng.uniform(-deg, deg))))\n",
    "    X = coords.copy(); p0 = X[pivot].copy()\n",
    "    X[pivot+1:e] = (X[pivot+1:e] - p0) @ R.T + p0\n",
    "    return X\n",
    "\n",
    "\n",
    "def jitter_chains(coords, segs, rng, deg=12, trans=1.5):\n",
    "    X = coords.copy(); gc_ = X.mean(0, keepdims=True)\n",
    "    for s, e in segs:\n",
    "        R     = _rotmat(rng.normal(size=3), np.deg2rad(float(rng.uniform(-deg, deg))))\n",
    "        shift = rng.normal(size=3); shift = shift / (np.linalg.norm(shift) + 1e-12) * float(rng.uniform(0, trans))\n",
    "        c     = X[s:e].mean(0, keepdims=True)\n",
    "        X[s:e] = (X[s:e] - c) @ R.T + c + shift\n",
    "    X -= X.mean(0, keepdims=True) - gc_\n",
    "    return X\n",
    "\n",
    "\n",
    "def smooth_wiggle(coords, segs, rng, amp=0.8):\n",
    "    X = coords.copy()\n",
    "    for s, e in segs:\n",
    "        L = e - s\n",
    "        if L < 20: continue\n",
    "        ctrl = np.linspace(0, L - 1, 6); disp = rng.normal(0, amp, (6, 3)); t = np.arange(L)\n",
    "        X[s:e] += np.vstack([np.interp(t, ctrl, disp[:, k]) for k in range(3)]).T\n",
    "    return X\n",
    "\n",
    "\n",
    "def generate_rna_structure(sequence: str, seed=None) -> np.ndarray:\n",
    "    \"\"\"Idealized A-form RNA helix — last-resort de-novo fallback.\"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    n = len(sequence); coords = np.zeros((n, 3))\n",
    "    for i in range(n):\n",
    "        ang = i * 0.6\n",
    "        coords[i] = [10.0 * np.cos(ang), 10.0 * np.sin(ang), i * 2.5]\n",
    "    return coords\n",
    "\n",
    "\n",
    "# ─────────────── TBM Phase ───────────────────────────────────────────────────\n",
    "def tbm_phase(test_df, train_seqs_df, train_coords_dict, segments_map):\n",
    "    \"\"\"\n",
    "    Phase 1 — Template-Based Modeling.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    template_predictions : {target_id: [np.ndarray(seq_len, 3), ...]}\n",
    "        0 to N_SAMPLE predictions per target, from real templates.\n",
    "    protenix_queue : {target_id: (n_needed, full_sequence)}\n",
    "        Targets that still need more predictions.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PHASE 1: Template-Based Modeling\")\n",
    "    print(f\"  MIN_SIMILARITY = {MIN_SIMILARITY}  |  MIN_PCT_IDENTITY = {MIN_PERCENT_IDENTITY}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    template_predictions: dict = {}\n",
    "    protenix_queue:       dict = {}\n",
    "\n",
    "    for _, row in test_df.iterrows():\n",
    "        tid = row[\"target_id\"]\n",
    "        seq = row[\"sequence\"]\n",
    "        segs = segments_map.get(tid, [(0, len(seq))])\n",
    "\n",
    "        similar = find_similar_sequences_detailed(seq, train_seqs_df, train_coords_dict, top_n=30)\n",
    "        preds   = []\n",
    "        used    = set()\n",
    "\n",
    "        for i, (tmpl_id, tmpl_seq, sim, tmpl_coords, pct_id, _, _) in enumerate(similar):\n",
    "            if len(preds) >= N_SAMPLE:\n",
    "                break\n",
    "            if sim < MIN_SIMILARITY or pct_id < MIN_PERCENT_IDENTITY:\n",
    "                break           # list is sorted by sim, so no point continuing\n",
    "            if tmpl_id in used:\n",
    "                continue\n",
    "\n",
    "            rng     = np.random.default_rng((row.name * 10000000000 + i * 10007) % (2**32))\n",
    "            adapted = adapt_template_to_query(seq, tmpl_seq, tmpl_coords)\n",
    "\n",
    "            # Diversity transforms (same strategy as the 0-409 TBM notebook)\n",
    "            slot = len(preds)\n",
    "            if slot == 0:\n",
    "                X = adapted\n",
    "            elif slot == 1:\n",
    "                X = adapted + rng.normal(0, max(0.01, (0.40 - sim) * 0.06), adapted.shape)\n",
    "            elif slot == 2:\n",
    "                longest = max(segs, key=lambda se: se[1] - se[0])\n",
    "                X = apply_hinge(adapted, longest, rng)\n",
    "            elif slot == 3:\n",
    "                X = jitter_chains(adapted, segs, rng)\n",
    "            else:\n",
    "                X = smooth_wiggle(adapted, segs, rng)\n",
    "\n",
    "            refined = adaptive_rna_constraints(X, tid, segments_map, confidence=sim)\n",
    "            preds.append(refined)\n",
    "            used.add(tmpl_id)\n",
    "\n",
    "        template_predictions[tid] = preds\n",
    "        n_needed = N_SAMPLE - len(preds)\n",
    "        if n_needed > 0:\n",
    "            protenix_queue[tid] = (n_needed, seq)\n",
    "            print(f\"  {tid} ({len(seq)} nt): {len(preds)} TBM → need {n_needed} from Protenix\")\n",
    "        else:\n",
    "            print(f\"  {tid} ({len(seq)} nt): all {N_SAMPLE} from TBM ✓\")\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    n_full  = len(test_df) - len(protenix_queue)\n",
    "    print(f\"\\nPhase 1 done in {elapsed:.1f}s\")\n",
    "    print(f\"  Fully covered by TBM : {n_full}\")\n",
    "    print(f\"  Need Protenix        : {len(protenix_queue)}\")\n",
    "    return template_predictions, protenix_queue\n",
    "\n",
    "\n",
    "# ─────────────── Main ────────────────────────────────────────────────────────\n",
    "def main() -> None:\n",
    "    test_csv, output_csv, code_dir, root_dir = resolve_paths()\n",
    "\n",
    "    if not os.path.isdir(code_dir):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Missing PROTENIX_CODE_DIR: {code_dir}. \"\n",
    "            \"Set PROTENIX_CODE_DIR to the repo path.\"\n",
    "        )\n",
    "\n",
    "    os.environ[\"PROTENIX_ROOT_DIR\"] = root_dir\n",
    "    sys.path.append(code_dir)\n",
    "    ensure_required_files(root_dir)\n",
    "    seed_everything(SEED)\n",
    "\n",
    "    # ── Load test data ──────────────────────────────────────────────────────\n",
    "    test_df_full = pd.read_csv(test_csv)\n",
    "    test_df      = (test_df_full.head(LOCAL_N_SAMPLES) if not IS_KAGGLE\n",
    "                    else test_df_full).reset_index(drop=True)\n",
    "    print(f\"Test targets : {len(test_df)}\"\n",
    "          + (\" (LOCAL MODE)\" if not IS_KAGGLE else \"\"))\n",
    "\n",
    "    seq_by_id = dict(zip(test_df[\"target_id\"], test_df[\"sequence\"]))\n",
    "\n",
    "    # Truncated copy for Protenix (Protenix has token limits)\n",
    "    test_df_trunc = test_df.copy()\n",
    "    test_df_trunc[\"sequence\"] = test_df_trunc[\"sequence\"].str[:MAX_SEQ_LEN]\n",
    "\n",
    "    # ── Load training data for TBM ──────────────────────────────────────────\n",
    "    print(\"\\nLoading training data for TBM …\")\n",
    "    train_seqs   = pd.read_csv(DEFAULT_TRAIN_CSV)\n",
    "    val_seqs     = pd.read_csv(DEFAULT_VAL_CSV)\n",
    "    train_labels = pd.read_csv(DEFAULT_TRAIN_LBLS)\n",
    "    val_labels   = pd.read_csv(DEFAULT_VAL_LBLS)\n",
    "\n",
    "    combined_seqs   = pd.concat([train_seqs,   val_seqs],    ignore_index=True)\n",
    "    combined_labels = pd.concat([train_labels, val_labels],  ignore_index=True)\n",
    "    train_coords    = process_labels(combined_labels)\n",
    "    segments_map, _ = build_segments_map(test_df)\n",
    "\n",
    "    print(f\"Template pool: {len(combined_seqs)} sequences, {len(train_coords)} structures\")\n",
    "\n",
    "    # ─── PHASE 1: TBM ──────────────────────────────────────────────────────\n",
    "    template_preds, protenix_queue = tbm_phase(\n",
    "        test_df, combined_seqs, train_coords, segments_map\n",
    "    )\n",
    "\n",
    "    # ─── PHASE 2: Protenix (only for targets that need extra predictions) ──\n",
    "    protenix_preds: dict = {}   # target_id -> np.ndarray (n_needed, seq_len, 3)\n",
    "\n",
    "    if protenix_queue and USE_PROTENIX:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"PHASE 2: Protenix for {len(protenix_queue)} targets\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        work_dir = Path(\"/kaggle/working\")\n",
    "        work_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Build input JSON only for queued targets\n",
    "        queue_df = (test_df_trunc[test_df_trunc[\"target_id\"].isin(protenix_queue)]\n",
    "                    .reset_index(drop=True))\n",
    "        input_json_path = str(work_dir / \"protenix_queue_input.json\")\n",
    "        build_input_json(queue_df, input_json_path)\n",
    "\n",
    "        from protenix.data.inference.infer_dataloader import InferenceDataset\n",
    "        from runner.inference import (InferenceRunner,\n",
    "                                      update_gpu_compatible_configs,\n",
    "                                      update_inference_configs)\n",
    "\n",
    "        configs = build_configs(input_json_path, str(work_dir / \"outputs\"), MODEL_NAME)\n",
    "        configs = update_gpu_compatible_configs(configs)\n",
    "        runner  = InferenceRunner(configs)\n",
    "        dataset = InferenceDataset(configs)\n",
    "\n",
    "        for i in tqdm(range(len(dataset)), desc=\"Protenix\"):\n",
    "            data, atom_array, error_message = dataset[i]\n",
    "            target_id = data.get(\"sample_name\", f\"sample_{i}\")\n",
    "\n",
    "            if target_id not in protenix_queue:\n",
    "                continue\n",
    "\n",
    "            n_needed, full_seq = protenix_queue[target_id]\n",
    "\n",
    "            if error_message:\n",
    "                print(f\"  {target_id}: data error — {error_message}\")\n",
    "                protenix_preds[target_id] = None\n",
    "                del data, atom_array, error_message\n",
    "                gc.collect(); torch.cuda.empty_cache(); gc.collect()\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                new_cfg = update_inference_configs(configs, data[\"N_token\"].item())\n",
    "                # Only generate as many samples as we actually need to fill the slots\n",
    "                new_cfg.sample_diffusion.N_sample = n_needed\n",
    "                runner.update_model_configs(new_cfg)\n",
    "\n",
    "                prediction = runner.predict(data)\n",
    "                raw_coords = prediction[\"coordinate\"] # Shape: [N_sample, all_atoms, 3]\n",
    "\n",
    "                # -----------------------------------------------------------\n",
    "                # DEBUG PRINT START\n",
    "                # -----------------------------------------------------------\n",
    "                print(f\"\\n[DEBUG] {target_id} | n_needed: {n_needed} | SeqLen: {len(full_seq)}\")\n",
    "                print(f\"[DEBUG] raw_coords shape: {raw_coords.shape}\")\n",
    "                \n",
    "                feat = data[\"input_feature_dict\"]\n",
    "                \n",
    "                # Check potential masks\n",
    "                mask_candidates = {}\n",
    "                if \"centre_atom_mask\" in feat:\n",
    "                    m = feat[\"centre_atom_mask\"]\n",
    "                    mask_candidates['centre_atom_mask'] = (m.sum().item(), m.shape)\n",
    "                \n",
    "                if \"atom_to_tokatom_idx\" in feat:\n",
    "                    idx_11 = (feat[\"atom_to_tokatom_idx\"] == 11).sum().item()\n",
    "                    idx_12 = (feat[\"atom_to_tokatom_idx\"] == 12).sum().item()\n",
    "                    mask_candidates['idx_11'] = idx_11\n",
    "                    mask_candidates['idx_12'] = idx_12\n",
    "                \n",
    "                print(f\"[DEBUG] Mask candidates counts: {mask_candidates}\")\n",
    "                # -----------------------------------------------------------\n",
    "                # DEBUG PRINT END\n",
    "                # -----------------------------------------------------------\n",
    "\n",
    "                # ─────────────────────────────────────────────────────────────\n",
    "                # DEBUG / FIX: Explicit C1' masking logic\n",
    "                # ─────────────────────────────────────────────────────────────\n",
    "                # Try to use 'centre_atom_mask' from features if possible\n",
    "                if \"centre_atom_mask\" in feat:\n",
    "                    mask = (feat[\"centre_atom_mask\"] == 1).to(raw_coords.device)\n",
    "                elif \"atom_to_tokatom_idx\" in feat:\n",
    "                    # Heuristic: pick the one closest to sequence length\n",
    "                    m11 = (feat[\"atom_to_tokatom_idx\"] == 11).to(raw_coords.device)\n",
    "                    m12 = (feat[\"atom_to_tokatom_idx\"] == 12).to(raw_coords.device)\n",
    "                    \n",
    "                    c11, c12 = m11.sum(), m12.sum()\n",
    "                    target_len = len(full_seq) # closer to N_token usually\n",
    "                    \n",
    "                    if abs(c11 - target_len) < abs(c12 - target_len):\n",
    "                         mask = m11\n",
    "                         print(f\"[DEBUG] Selected idx 11 mask (count={c11})\")\n",
    "                    else:\n",
    "                         mask = m12\n",
    "                         print(f\"[DEBUG] Selected idx 12 mask (count={c12})\")\n",
    "                else:\n",
    "                    # Should not happen\n",
    "                    mask = torch.zeros(raw_coords.shape[1], dtype=torch.bool, device=raw_coords.device)\n",
    "                \n",
    "                # Extract\n",
    "                coords = raw_coords[:, mask, :].detach().cpu().numpy()\n",
    "                print(f\"[DEBUG] Extracted coords shape: {coords.shape}\")\n",
    "\n",
    "                # If we get duplicate coordinates (collapsed), this is bad.\n",
    "                # Check for duplications in first sample\n",
    "                if coords.shape[1] > 1:\n",
    "                     diffs = np.linalg.norm(coords[0, 1:] - coords[0, :-1], axis=-1)\n",
    "                     if np.all(diffs < 1e-4):\n",
    "                         print(f\"  WARNING: {target_id} has identical coordinates for all residues! (Model collapse?)\")\n",
    "                \n",
    "                # Pad/trim to full (un-truncated) sequence length\n",
    "                if coords.shape[1] != len(full_seq):\n",
    "                    # Check for broadcast issue or model collapse\n",
    "                    if coords.shape[1] == 1 and len(full_seq) > 1:\n",
    "                        # Model outputted only 1 residue/atom but we need many?\n",
    "                        # Broadcast the single coord to all positions just in case (though highly suspicious)\n",
    "                        # Or perhaps mask was wrong and selected only 1 atom.\n",
    "                        # Do NOT broadcast, fill with zeros to be safe.\n",
    "                        print(f\"[DEBUG] WARNING: {target_id}: mask selected only 1 atom, but sequence is {len(full_seq)}\")\n",
    "                        # padded = np.zeros(...) -> kept as zeros\n",
    "                    else:\n",
    "                        padded  = np.zeros((coords.shape[0], len(full_seq), 3), dtype=np.float32)\n",
    "                        min_len = min(coords.shape[1], len(full_seq))\n",
    "                        if min_len > 0:\n",
    "                            padded[:, :min_len, :] = coords[:, :min_len, :]\n",
    "                        coords = padded\n",
    "\n",
    "                # Final check for identical coordinates (indicative of model failure)\n",
    "                if coords.shape[1] > 1:\n",
    "                     diffs = np.linalg.norm(coords[0, 1:] - coords[0, :-1], axis=-1)\n",
    "                     if np.all(diffs < 1e-4):\n",
    "                         print(f\"  WARNING: {target_id}: Identical coordinates detected! Resetting to zeros.\")\n",
    "                         coords = np.zeros_like(coords)\n",
    "\n",
    "                protenix_preds[target_id] = coords\n",
    "                print(f\"  {target_id}: {coords.shape[0]} Protenix predictions generated\")\n",
    "\n",
    "            except Exception as exc:\n",
    "                print(f\"  {target_id}: Protenix FAILED — {exc}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                protenix_preds[target_id] = None\n",
    "\n",
    "            finally:\n",
    "                del prediction, raw_coords, mask, data, atom_array\n",
    "                gc.collect(); torch.cuda.empty_cache(); gc.collect()\n",
    "# ...existing code...\n",
    "\n",
    "    elif protenix_queue and not USE_PROTENIX:\n",
    "        print(f\"\\nPHASE 2 skipped (USE_PROTENIX=False). \"\n",
    "              f\"De-novo fallback will cover {len(protenix_queue)} targets.\")\n",
    "\n",
    "    # ─── PHASE 3: Combine everything ───────────────────────────────────────\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"PHASE 3: Combine TBM + Protenix + de-novo fallback\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    all_rows = []\n",
    "\n",
    "    for _, row in test_df.iterrows():\n",
    "        tid = row[\"target_id\"]\n",
    "        seq = row[\"sequence\"]\n",
    "\n",
    "        combined: list = list(template_preds.get(tid, []))  # TBM predictions\n",
    "\n",
    "        # Append Protenix predictions to fill remaining slots\n",
    "        ptx = protenix_preds.get(tid)\n",
    "        if ptx is not None and ptx.ndim == 3:\n",
    "            for j in range(ptx.shape[0]):\n",
    "                if len(combined) >= N_SAMPLE:\n",
    "                    break\n",
    "                combined.append(ptx[j])  # (seq_len, 3)\n",
    "\n",
    "        # De-novo fallback for any still-empty slots\n",
    "        n_denovo = 0\n",
    "        while len(combined) < N_SAMPLE:\n",
    "            seed_val = row.name * 1000000 + len(combined) * 1000\n",
    "            dn       = generate_rna_structure(seq, seed=seed_val)\n",
    "            combined.append(adaptive_rna_constraints(dn, tid, segments_map, confidence=0.2))\n",
    "            n_denovo += 1\n",
    "\n",
    "        if n_denovo:\n",
    "            print(f\"  {tid}: {n_denovo} slot(s) filled with de-novo fallback\")\n",
    "\n",
    "        # Stack to (N_SAMPLE, seq_len, 3) and write rows\n",
    "        stacked = np.stack(combined[:N_SAMPLE], axis=0)\n",
    "        all_rows.extend(coords_to_rows(tid, seq, stacked))\n",
    "\n",
    "    # ── Save ───────────────────────────────────────────────────────────────\n",
    "    sub = pd.DataFrame(all_rows)\n",
    "    cols = [\"ID\", \"resname\", \"resid\"] + [\n",
    "        f\"{c}_{i}\" for i in range(1, N_SAMPLE + 1) for c in [\"x\", \"y\", \"z\"]\n",
    "    ]\n",
    "    coord_cols = [c for c in cols if c.startswith((\"x_\", \"y_\", \"z_\"))]\n",
    "    sub[coord_cols] = sub[coord_cols].clip(-999.999, 9999.999)\n",
    "    sub[cols].to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\"\\n✓ Saved submission to {output_csv}  ({len(sub):,} rows)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425ca040",
   "metadata": {
    "papermill": {
     "duration": 0.002205,
     "end_time": "2026-02-22T09:11:52.784548",
     "exception": false,
     "start_time": "2026-02-22T09:11:52.782343",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f2ee97e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T09:11:52.789867Z",
     "iopub.status.busy": "2026-02-22T09:11:52.789632Z",
     "iopub.status.idle": "2026-02-22T09:14:30.947747Z",
     "shell.execute_reply": "2026-02-22T09:14:30.946891Z"
    },
    "papermill": {
     "duration": 158.162554,
     "end_time": "2026-02-22T09:14:30.949291",
     "exception": false,
     "start_time": "2026-02-22T09:11:52.786737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test targets : 2 (LOCAL MODE)\n",
      "\n",
      "Loading training data for TBM …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41/3035017873.py:533: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_labels = pd.read_csv(DEFAULT_TRAIN_LBLS)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template pool: 5744 sequences, 5744 structures\n",
      "\n",
      "============================================================\n",
      "PHASE 1: Template-Based Modeling\n",
      "  MIN_SIMILARITY = 0.0  |  MIN_PCT_IDENTITY = 50.0\n",
      "============================================================\n",
      "  8ZNQ (30 nt): 2 TBM → need 3 from Protenix\n",
      "  9IWF (69 nt): all 5 from TBM ✓\n",
      "\n",
      "Phase 1 done in 1.4s\n",
      "  Fully covered by TBM : 1\n",
      "  Need Protenix        : 1\n",
      "\n",
      "============================================================\n",
      "PHASE 2: Protenix for 1 targets\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-22 09:12:31,265 [/kaggle/input/datasets/qiweiyin/protenix-v1-adjusted/Protenix-v1-adjust-v2/Protenix-v1-adjust-v2/Protenix-v1/runner/inference.py:246] INFO runner.inference: Distributed environment: world size: 1, global rank: 0, local rank: 0\n",
      "2026-02-22 09:12:31,266 [/kaggle/input/datasets/qiweiyin/protenix-v1-adjusted/Protenix-v1-adjust-v2/Protenix-v1-adjust-v2/Protenix-v1/runner/inference.py:98] INFO root: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "2026-02-22 09:12:31,383 [/kaggle/input/datasets/qiweiyin/protenix-v1-adjusted/Protenix-v1-adjust-v2/Protenix-v1-adjust-v2/Protenix-v1/runner/inference.py:127] INFO root: Finished environment initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train scheduler 16.0\n",
      "inference scheduler 16.0\n",
      "Diffusion Module has 16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-22 09:13:40,219 [/kaggle/input/datasets/qiweiyin/protenix-v1-adjusted/Protenix-v1-adjust-v2/Protenix-v1-adjust-v2/Protenix-v1/runner/inference.py:246] INFO runner.inference: Loading from /kaggle/input/datasets/qiweiyin/protenix-v1-adjusted/Protenix-v1-adjust-v2/Protenix-v1-adjust-v2/Protenix-v1/checkpoint/protenix_base_20250630_v1.0.0.pt, strict: True\n",
      "2026-02-22 09:13:54,332 [/kaggle/input/datasets/qiweiyin/protenix-v1-adjusted/Protenix-v1-adjust-v2/Protenix-v1-adjust-v2/Protenix-v1/runner/inference.py:246] INFO runner.inference: Sampled key: module.input_embedder.atom_attention_encoder.linear_no_bias_ref_pos.weight\n",
      "2026-02-22 09:13:54,469 [/kaggle/input/datasets/qiweiyin/protenix-v1-adjusted/Protenix-v1-adjust-v2/Protenix-v1-adjust-v2/Protenix-v1/runner/inference.py:246] INFO runner.inference: Finish loading checkpoint.\n",
      "2026-02-22 09:13:54,479 [/kaggle/input/datasets/qiweiyin/protenix-v1-adjusted/Protenix-v1-adjust-v2/Protenix-v1-adjust-v2/Protenix-v1/runner/inference.py:246] INFO runner.inference: Model parameters: 368.48M\n",
      "Protenix:   0%|          | 0/1 [00:00<?, ?it/s]2026-02-22 09:13:54,516 [/kaggle/input/datasets/qiweiyin/protenix-v1-adjusted/Protenix-v1-adjust-v2/Protenix-v1-adjust-v2/Protenix-v1/protenix/data/inference/infer_dataloader.py:281] INFO protenix.data.inference.infer_dataloader: Featurizing 8ZNQ...\n",
      "2026-02-22 09:14:05,891 [/kaggle/input/datasets/qiweiyin/protenix-v1-adjusted/Protenix-v1-adjust-v2/Protenix-v1-adjust-v2/Protenix-v1/protenix/data/constraint/constraint_featurizer.py:392] INFO protenix.data.constraint.constraint_featurizer: Loaded constraint feature: #atom contact:0 #contact:0 #pocket:0\n",
      "2026-02-22 09:14:05,934 [/kaggle/input/datasets/qiweiyin/protenix-v1-adjusted/Protenix-v1-adjust-v2/Protenix-v1-adjust-v2/Protenix-v1/protenix/data/template/template_featurizer.py:668] INFO protenix.data.template.template_featurizer: Calling InferenceTemplateFeaturizer.make_template_feature\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DEBUG] 8ZNQ | n_needed: 3 | SeqLen: 30\n",
      "[DEBUG] raw_coords shape: torch.Size([3, 639, 3])\n",
      "[DEBUG] Mask candidates counts: {'idx_11': 30, 'idx_12': 30}\n",
      "[DEBUG] Selected idx 12 mask (count=30)\n",
      "[DEBUG] Extracted coords shape: (3, 30, 3)\n",
      "  8ZNQ: 3 Protenix predictions generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Protenix: 100%|██████████| 1/1 [00:36<00:00, 36.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PHASE 3: Combine TBM + Protenix + de-novo fallback\n",
      "============================================================\n",
      "\n",
      "✓ Saved submission to /kaggle/working/submission.csv  (99 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec3fcfeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T09:14:30.956936Z",
     "iopub.status.busy": "2026-02-22T09:14:30.956691Z",
     "iopub.status.idle": "2026-02-22T09:14:30.972159Z",
     "shell.execute_reply": "2026-02-22T09:14:30.971436Z"
    },
    "papermill": {
     "duration": 0.020805,
     "end_time": "2026-02-22T09:14:30.973441",
     "exception": false,
     "start_time": "2026-02-22T09:14:30.952636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ID resname  resid        x_1        y_1        z_1         x_2  \\\n",
      "0    8ZNQ_1       A      1  -2.054246 -15.061790  20.740161  144.905975   \n",
      "1    8ZNQ_2       C      2  -1.971328 -15.075652  15.338831  141.453103   \n",
      "2    8ZNQ_3       C      3  -3.348014 -13.500002  10.449140  138.508506   \n",
      "3    8ZNQ_4       G      4  -5.444428 -11.039888   6.602210  142.583253   \n",
      "4    8ZNQ_5       U      5  -6.349629  -6.257964   4.544753  143.602598   \n",
      "5    8ZNQ_6       G      6  -6.918016  -0.888219   3.240932  142.421563   \n",
      "6    8ZNQ_7       A      7  -4.490872   4.125901  -0.914170  139.531180   \n",
      "7    8ZNQ_8       C      8  -5.403762   9.778472   3.356724  137.092470   \n",
      "8    8ZNQ_9       G      9  -0.394198  10.589010   0.112758  135.599965   \n",
      "9   8ZNQ_10       G     10   3.902270   9.966390  -2.052086  134.618762   \n",
      "10  8ZNQ_11       G     11   7.968209   7.519009  -4.491783  133.472268   \n",
      "11  8ZNQ_12       C     12  10.186732   4.490251  -9.001780  132.259513   \n",
      "12  8ZNQ_13       C     13   9.848646   3.063519 -14.389865  131.430724   \n",
      "13  8ZNQ_14       U     14   7.519880   4.436866 -18.485039  129.839686   \n",
      "14  8ZNQ_15       U     15   8.376199  11.915060 -21.834311  128.055502   \n",
      "15  8ZNQ_16       U     16   0.130919  11.237054 -20.965179  130.135670   \n",
      "16  8ZNQ_17       U     17  -3.649398   8.230392 -15.675855  138.936029   \n",
      "17  8ZNQ_18       G     18  -0.864956   2.149688 -16.331209  144.306915   \n",
      "18  8ZNQ_19       G     19   2.349325  -0.740085 -13.500938  147.476884   \n",
      "19  8ZNQ_20       C     20   5.657008  -1.855106  -9.242171  149.427930   \n",
      "\n",
      "           y_2         z_2        x_3        y_3       z_3       x_4  \\\n",
      "0   185.056087  167.756089 -17.051006   3.635357  4.948226 -4.738762   \n",
      "1   184.779583  167.005882 -15.824107   0.381011  0.995712  0.168242   \n",
      "2   185.206089  168.080214 -13.090235  -0.863414 -3.801722  4.918828   \n",
      "3   187.065217  172.800784  -8.658511  -0.040837 -6.868982  7.256896   \n",
      "4   190.496283  176.421094  -3.799119   1.918953 -6.934116  6.405554   \n",
      "5   194.726039  179.341957   0.911164   4.269243 -5.171895  3.540940   \n",
      "6   198.787146  180.882346   6.150577   5.965312 -3.323246 -0.656656   \n",
      "7   199.638805  180.036130   8.003081   5.785764  1.475034 -5.998435   \n",
      "8   199.672594  178.960757   7.685435   3.355617  5.920546 -8.827372   \n",
      "9   199.777934  177.940717   6.052866  -1.197249  8.931765 -8.805513   \n",
      "10  199.592561  177.394008   5.391689  -6.703268  9.980293 -6.929714   \n",
      "11  200.026213  177.128322   6.822661 -11.679197  8.671481 -3.134614   \n",
      "12  200.105358  176.033656  10.006501 -14.899166  5.512016  1.365722   \n",
      "13  200.351512  175.929425  14.534092 -15.033489  2.530274  4.086778   \n",
      "14  200.472490  174.867377  22.360279 -15.821974  2.123485  4.120439   \n",
      "15  200.521960  172.368957  20.962547 -12.509354 -1.777556  3.714627   \n",
      "16  200.117258  173.168522  18.756706 -10.168086 -6.599988  8.613798   \n",
      "17  197.161235  171.800823  14.098961 -10.553707 -3.268465  7.424307   \n",
      "18  192.571345  169.435268   9.119484 -12.736341 -1.557321  6.500343   \n",
      "19  189.154811  164.301186   4.423770 -12.443194  1.163438  3.508233   \n",
      "\n",
      "          y_4        z_4        x_5        y_5        z_5  \n",
      "0    8.888163 -14.393833  14.318347   8.743946   8.523230  \n",
      "1    6.705906 -14.019563  15.505487   4.387413   5.778694  \n",
      "2    6.257166 -11.029125  14.607524  -1.119827   4.716581  \n",
      "3    6.692606  -6.077417  11.075745  -5.253675   5.130246  \n",
      "4    6.925520  -0.930855   6.103228  -6.632076   5.948391  \n",
      "5    6.908742   3.792991   0.556264  -6.162644   6.725566  \n",
      "6    5.314388   5.996765  -3.386962  -3.153783   5.892482  \n",
      "7    4.055926   8.742867  -7.610548   0.773899   5.384146  \n",
      "8   -0.158025   7.928111  -8.384164   4.379174   1.978279  \n",
      "9   -5.075037   5.373202  -6.750369   6.497382  -3.053155  \n",
      "10  -9.994596   3.244857  -5.235620   6.277015  -8.395588  \n",
      "11 -13.775562   3.393780  -5.071981   3.369624 -12.857809  \n",
      "12 -16.016531   5.666874  -6.423538  -1.358047 -15.273158  \n",
      "13 -16.308369  10.106125  -9.694122  -5.551484 -15.465781  \n",
      "14 -19.238991  17.454075 -17.211859  -8.820697 -16.499491  \n",
      "15 -12.959419  15.439795 -15.526757 -12.238614 -10.730846  \n",
      "16  -7.318677  19.056480 -13.699848 -14.596642  -6.668367  \n",
      "17  -9.668119  11.695068  -8.705992  -9.605645  -8.927528  \n",
      "18 -10.628927   6.136163  -3.951390  -6.941509 -11.172408  \n",
      "19 -10.299995   1.530129  -0.453392  -2.734304 -10.893084  \n"
     ]
    }
   ],
   "source": [
    "#read submission.csv\n",
    "submission_path = \"/kaggle/working/submission.csv\"\n",
    "submission_df = pd.read_csv(submission_path)\n",
    "print(submission_df.head(20))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 15231210,
     "sourceId": 118765,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 15440074,
     "datasetId": 9328538,
     "sourceId": 14604295,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 15736806,
     "datasetId": 9502242,
     "sourceId": 14874339,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 11219268,
     "datasetId": 6742586,
     "sourceId": 10855324,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 168.050338,
   "end_time": "2026-02-22T09:14:34.109862",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-22T09:11:46.059524",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
