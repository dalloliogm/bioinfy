{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81426033",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T21:41:28.961002Z",
     "iopub.status.busy": "2026-02-20T21:41:28.960647Z",
     "iopub.status.idle": "2026-02-20T21:41:30.077072Z",
     "shell.execute_reply": "2026-02-20T21:41:30.076076Z"
    },
    "papermill": {
     "duration": 1.125066,
     "end_time": "2026-02-20T21:41:30.079693",
     "exception": false,
     "start_time": "2026-02-20T21:41:28.954627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in LOCAL mode — only the first 2 test targets will be processed to save time.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "IS_KAGGLE = bool(os.environ.get(\"KAGGLE_IS_COMPETITION_RERUN\", \"\"))\n",
    "\n",
    "# How many test samples to use when running locally\n",
    "LOCAL_N_SAMPLES = 2\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    print(\"Running in KAGGLE COMPETITION mode — all test targets will be processed.\")\n",
    "else:\n",
    "    print(f\"Running in LOCAL mode — only the first {LOCAL_N_SAMPLES} test targets \"\n",
    "          f\"will be processed to save time.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98d51930",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T21:41:30.090069Z",
     "iopub.status.busy": "2026-02-20T21:41:30.089240Z",
     "iopub.status.idle": "2026-02-20T21:41:34.888027Z",
     "shell.execute_reply": "2026-02-20T21:41:34.886670Z"
    },
    "papermill": {
     "duration": 4.80658,
     "end_time": "2026-02-20T21:41:34.890291",
     "exception": false,
     "start_time": "2026-02-20T21:41:30.083711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "os.environ[\"LAYERNORM_TYPE\"] = \"torch\"\n",
    "os.environ.setdefault(\"RNA_MSA_DEPTH_LIMIT\", \"512\")\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from Bio.Align import PairwiseAligner\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2ebad3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T21:41:34.900150Z",
     "iopub.status.busy": "2026-02-20T21:41:34.899616Z",
     "iopub.status.idle": "2026-02-20T21:41:34.909488Z",
     "shell.execute_reply": "2026-02-20T21:41:34.908313Z"
    },
    "papermill": {
     "duration": 0.017418,
     "end_time": "2026-02-20T21:41:34.911600",
     "exception": false,
     "start_time": "2026-02-20T21:41:34.894182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_c1_mask(data: dict, atom_array) -> torch.Tensor:\n",
    "    # 1. Try atom_array attributes first\n",
    "    if atom_array is not None:\n",
    "        try:\n",
    "            if hasattr(atom_array, \"centre_atom_mask\"):\n",
    "                m = atom_array.centre_atom_mask == 1\n",
    "                if hasattr(atom_array, \"is_rna\"):\n",
    "                    m = m & atom_array.is_rna\n",
    "                return torch.from_numpy(m).bool()\n",
    "            \n",
    "            if hasattr(atom_array, \"atom_name\"):\n",
    "                base = atom_array.atom_name == \"C1'\"\n",
    "                if hasattr(atom_array, \"is_rna\"):\n",
    "                    base = base & atom_array.is_rna\n",
    "                return torch.from_numpy(base).bool()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 2. Fallback to feature dict\n",
    "    f = data[\"input_feature_dict\"]\n",
    "    \n",
    "    if \"centre_atom_mask\" in f:\n",
    "        return (f[\"centre_atom_mask\"] == 1).bool()\n",
    "    if \"center_atom_mask\" in f:\n",
    "        return (f[\"center_atom_mask\"] == 1).bool()\n",
    "        \n",
    "    # Heuristic fallback: check which index gives us roughly N_token atoms\n",
    "    n_tokens = data.get(\"N_token\", torch.tensor(0)).item()\n",
    "    mask11 = (f[\"atom_to_tokatom_idx\"] == 11).bool()\n",
    "    mask12 = (f[\"atom_to_tokatom_idx\"] == 12).bool()\n",
    "    \n",
    "    c11 = mask11.sum().item()\n",
    "    c12 = mask12.sum().item()\n",
    "    \n",
    "    # Return the one closer to N_tokens (likely one per residue)\n",
    "    if abs(c11 - n_tokens) < abs(c12 - n_tokens):\n",
    "        return mask11\n",
    "    else:\n",
    "        return mask12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "087ff4bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T21:41:34.921902Z",
     "iopub.status.busy": "2026-02-20T21:41:34.921341Z",
     "iopub.status.idle": "2026-02-20T21:41:35.035311Z",
     "shell.execute_reply": "2026-02-20T21:41:35.034420Z"
    },
    "papermill": {
     "duration": 0.122343,
     "end_time": "2026-02-20T21:41:35.037682",
     "exception": false,
     "start_time": "2026-02-20T21:41:34.915339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/Bio/Align/__init__.py:4414: BiopythonDeprecationWarning: The attribute 'query_left_open_gap_score' was renamed to 'open_left_deletion_score'. This was done to be consistent with the\n",
      "AlignmentCounts object returned by the .counts method of an Alignment object.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/Bio/Align/__init__.py:4414: BiopythonDeprecationWarning: The attribute 'query_left_extend_gap_score' was renamed to 'extend_left_deletion_score'. This was done to be consistent with the\n",
      "AlignmentCounts object returned by the .counts method of an Alignment object.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/Bio/Align/__init__.py:4414: BiopythonDeprecationWarning: The attribute 'query_right_open_gap_score' was renamed to 'open_right_deletion_score'. This was done to be consistent with the\n",
      "AlignmentCounts object returned by the .counts method of an Alignment object.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/Bio/Align/__init__.py:4414: BiopythonDeprecationWarning: The attribute 'query_right_extend_gap_score' was renamed to 'extend_right_deletion_score'. This was done to be consistent with the\n",
      "AlignmentCounts object returned by the .counts method of an Alignment object.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/Bio/Align/__init__.py:4414: BiopythonDeprecationWarning: The attribute 'target_left_open_gap_score' was renamed to 'open_left_insertion_score'. This was done to be consistent with the\n",
      "AlignmentCounts object returned by the .counts method of an Alignment object.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/Bio/Align/__init__.py:4414: BiopythonDeprecationWarning: The attribute 'target_left_extend_gap_score' was renamed to 'extend_left_insertion_score'. This was done to be consistent with the\n",
      "AlignmentCounts object returned by the .counts method of an Alignment object.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/Bio/Align/__init__.py:4414: BiopythonDeprecationWarning: The attribute 'target_right_open_gap_score' was renamed to 'open_right_insertion_score'. This was done to be consistent with the\n",
      "AlignmentCounts object returned by the .counts method of an Alignment object.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/Bio/Align/__init__.py:4414: BiopythonDeprecationWarning: The attribute 'target_right_extend_gap_score' was renamed to 'extend_right_insertion_score'. This was done to be consistent with the\n",
      "AlignmentCounts object returned by the .counts method of an Alignment object.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ─────────────── Paths & Constants ───────────────────────────────────────────\n",
    "DATA_BASE              = \"/kaggle/input/stanford-rna-3d-folding-2\"\n",
    "DEFAULT_TEST_CSV       = f\"{DATA_BASE}/test_sequences.csv\"\n",
    "DEFAULT_TRAIN_CSV      = f\"{DATA_BASE}/train_sequences.csv\"\n",
    "DEFAULT_TRAIN_LBLS     = f\"{DATA_BASE}/train_labels.csv\"\n",
    "DEFAULT_VAL_CSV        = f\"{DATA_BASE}/validation_sequences.csv\"\n",
    "DEFAULT_VAL_LBLS       = f\"{DATA_BASE}/validation_labels.csv\"\n",
    "DEFAULT_OUTPUT         = \"/kaggle/working/submission.csv\"\n",
    "\n",
    "DEFAULT_CODE_DIR = (\n",
    "    \"/kaggle/input/datasets/qiweiyin/protenix-v1-adjusted\"\n",
    "    \"/Protenix-v1-adjust-v2/Protenix-v1-adjust-v2/Protenix-v1\"\n",
    ")\n",
    "DEFAULT_ROOT_DIR = DEFAULT_CODE_DIR\n",
    "\n",
    "MODEL_NAME    = \"protenix_base_20250630_v1.0.0\"\n",
    "N_SAMPLE      = 7\n",
    "SEED          = 42\n",
    "MAX_SEQ_LEN   = int(os.environ.get(\"MAX_SEQ_LEN\",   \"512\"))\n",
    "CHUNK_OVERLAP = int(os.environ.get(\"CHUNK_OVERLAP\",  \"64\"))\n",
    "\n",
    "# TBM quality thresholds — sequences below these get routed to Protenix\n",
    "MIN_SIMILARITY       = float(os.environ.get(\"MIN_SIMILARITY\",       \"0.0\"))\n",
    "MIN_PERCENT_IDENTITY = float(os.environ.get(\"MIN_PERCENT_IDENTITY\", \"50.0\"))\n",
    "\n",
    "# Set False to skip Protenix and use de-novo fallback instead\n",
    "USE_PROTENIX = True\n",
    "\n",
    "\n",
    "def parse_bool(value: str, default: bool = False) -> str:\n",
    "    v = str(value).strip().lower()\n",
    "    if v in {\"1\", \"true\", \"t\", \"yes\", \"y\", \"on\"}:\n",
    "        return \"true\"\n",
    "    if v in {\"0\", \"false\", \"f\", \"no\", \"n\", \"off\"}:\n",
    "        return \"false\"\n",
    "    return \"true\" if default else \"false\"\n",
    "\n",
    "\n",
    "USE_MSA      = parse_bool(os.environ.get(\"USE_MSA\",      \"false\"))\n",
    "USE_TEMPLATE = parse_bool(os.environ.get(\"USE_TEMPLATE\", \"true\"))\n",
    "USE_RNA_MSA  = parse_bool(os.environ.get(\"USE_RNA_MSA\",  \"true\"))\n",
    "\n",
    "MODEL_N_SAMPLE = int(os.environ.get(\"MODEL_N_SAMPLE\", str(N_SAMPLE)))\n",
    "\n",
    "\n",
    "# ─────────────── General Utilities ───────────────────────────────────────────\n",
    "def seed_everything(seed: int) -> None:\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "def resolve_paths():\n",
    "    test_csv   = os.environ.get(\"TEST_CSV\",           DEFAULT_TEST_CSV)\n",
    "    output_csv = os.environ.get(\"SUBMISSION_CSV\",     DEFAULT_OUTPUT)\n",
    "    code_dir   = os.environ.get(\"PROTENIX_CODE_DIR\",  DEFAULT_CODE_DIR)\n",
    "    \n",
    "    # FIX: Check if we created a writable root in previous cells\n",
    "    if os.path.isdir(\"/kaggle/working/protenix_data\"):\n",
    "        root_dir = \"/kaggle/working/protenix_data\"\n",
    "    elif os.path.isdir(\"/kaggle/working/protenix_root\"):\n",
    "        root_dir = \"/kaggle/working/protenix_root\"\n",
    "    else:\n",
    "        root_dir = os.environ.get(\"PROTENIX_ROOT_DIR\",  DEFAULT_ROOT_DIR)\n",
    "        \n",
    "    return test_csv, output_csv, code_dir, root_dir\n",
    "\n",
    "\n",
    "def ensure_required_files(root_dir: str) -> None:\n",
    "    for p, name in [\n",
    "        (Path(root_dir) / \"checkpoint\" / f\"{MODEL_NAME}.pt\",          \"checkpoint\"),\n",
    "        (Path(root_dir) / \"common\" / \"components.cif\",                \"CCD file\"),\n",
    "        (Path(root_dir) / \"common\" / \"components.cif.rdkit_mol.pkl\",  \"CCD cache\"),\n",
    "    ]:\n",
    "        if not p.exists():\n",
    "            raise FileNotFoundError(f\"Missing {name}: {p}\")\n",
    "\n",
    "\n",
    "# ─────────────── Protenix Input / Config Helpers ─────────────────────────────\n",
    "def build_input_json(df: pd.DataFrame, json_path: str) -> None:\n",
    "    data = [\n",
    "        {\n",
    "            \"name\": row[\"target_id\"],\n",
    "            \"covalent_bonds\": [],\n",
    "            \"sequences\": [{\"rnaSequence\": {\"sequence\": row[\"sequence\"], \"count\": 1}}],\n",
    "        }\n",
    "        for _, row in df.iterrows()\n",
    "    ]\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "\n",
    "def build_configs(input_json_path: str, dump_dir: str, model_name: str):\n",
    "    from configs.configs_base import configs as configs_base\n",
    "    from configs.configs_data import data_configs\n",
    "    from configs.configs_inference import inference_configs\n",
    "    from configs.configs_model_type import model_configs\n",
    "    from protenix.config.config import parse_configs\n",
    "\n",
    "    base = {**configs_base, **{\"data\": data_configs}, **inference_configs}\n",
    "\n",
    "    def deep_update(t, p):\n",
    "        for k, v in p.items():\n",
    "            if isinstance(v, dict) and k in t and isinstance(t[k], dict):\n",
    "                deep_update(t[k], v)\n",
    "            else:\n",
    "                t[k] = v\n",
    "\n",
    "    deep_update(base, model_configs[model_name])\n",
    "    arg_str = \" \".join([\n",
    "        f\"--model_name {model_name}\",\n",
    "        f\"--input_json_path {input_json_path}\",\n",
    "        f\"--dump_dir {dump_dir}\",\n",
    "        f\"--use_msa {USE_MSA}\",\n",
    "        f\"--use_template {USE_TEMPLATE}\",\n",
    "        f\"--use_rna_msa {USE_RNA_MSA}\",\n",
    "        f\"--sample_diffusion.N_sample {MODEL_N_SAMPLE}\",\n",
    "        f\"--seeds {SEED}\",\n",
    "    ])\n",
    "    return parse_configs(configs=base, arg_str=arg_str, fill_required_with_null=True)\n",
    "\n",
    "\n",
    "def get_c1_mask(data: dict, atom_array) -> torch.Tensor:\n",
    "    # 1. Try atom_array attributes first\n",
    "    if atom_array is not None:\n",
    "        try:\n",
    "            if hasattr(atom_array, \"centre_atom_mask\"):\n",
    "                m = atom_array.centre_atom_mask == 1\n",
    "                if hasattr(atom_array, \"is_rna\"):\n",
    "                    m = m & atom_array.is_rna\n",
    "                return torch.from_numpy(m).bool()\n",
    "            \n",
    "            if hasattr(atom_array, \"atom_name\"):\n",
    "                base = atom_array.atom_name == \"C1'\"\n",
    "                if hasattr(atom_array, \"is_rna\"):\n",
    "                    base = base & atom_array.is_rna\n",
    "                return torch.from_numpy(base).bool()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 2. Fallback to feature dict\n",
    "    f = data[\"input_feature_dict\"]\n",
    "    \n",
    "    # CASE A: center_atom_mask exists\n",
    "    if \"center_atom_mask\" in f:\n",
    "        return (f[\"center_atom_mask\"] == 1).bool()\n",
    "    if \"centre_atom_mask\" in f:\n",
    "        return (f[\"centre_atom_mask\"] == 1).bool()\n",
    "        \n",
    "    # CASE B: Use atom_name\n",
    "    if \"atom_name\" in f:\n",
    "        # Check against \"C1'\" (byte encoded or string?)\n",
    "        # For now assume typical behavior is center_atom_mask is present.\n",
    "        pass\n",
    "\n",
    "    # CASE C: atom_to_tokatom_idx fallback\n",
    "    # The index for C1' is typically 11 or 12 depending on featurizer.\n",
    "    # Let's try to match exactly C1' if possible.\n",
    "    # But usually 'centre_atom_mask' should be there.\n",
    "    \n",
    "    # If we fall through, assume standard mask\n",
    "    return (f[\"atom_to_tokatom_idx\"] == 11).bool()\n",
    "\n",
    "\n",
    "def get_feature_c1_mask(data: dict) -> torch.Tensor:\n",
    "    f = data[\"input_feature_dict\"]\n",
    "    if \"centre_atom_mask\" in f:\n",
    "        return f[\"centre_atom_mask\"].long() == 1\n",
    "    return f[\"atom_to_tokatom_idx\"].long() == 12\n",
    "\n",
    "\n",
    "def coords_to_rows(target_id: str, seq: str, coords: np.ndarray) -> list:\n",
    "    \"\"\"coords shape: (N_SAMPLE, seq_len, 3)\"\"\"\n",
    "    rows = []\n",
    "    for i in range(len(seq)):\n",
    "        row = {\"ID\": f\"{target_id}_{i + 1}\", \"resname\": seq[i], \"resid\": i + 1}\n",
    "        for s in range(N_SAMPLE):\n",
    "            if s < coords.shape[0] and i < coords.shape[1]:\n",
    "                x, y, z = coords[s, i]\n",
    "            else:\n",
    "                x, y, z = 0.0, 0.0, 0.0\n",
    "            row[f\"x_{s + 1}\"] = float(x)\n",
    "            row[f\"y_{s + 1}\"] = float(y)\n",
    "            row[f\"z_{s + 1}\"] = float(z)\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "\n",
    "def pad_samples(coords: np.ndarray, n: int) -> np.ndarray:\n",
    "    if coords.shape[0] >= n:\n",
    "        return coords[:n]\n",
    "    if coords.shape[0] == 0:\n",
    "        return np.zeros((n, coords.shape[1], 3), dtype=coords.dtype)\n",
    "    extra = np.repeat(coords[:1], n - coords.shape[0], axis=0)\n",
    "    return np.concatenate([coords, extra], axis=0)\n",
    "\n",
    "\n",
    "# ─────────────── TBM Core Functions ──────────────────────────────────────────\n",
    "def _make_aligner() -> PairwiseAligner:\n",
    "    al = PairwiseAligner()\n",
    "    al.mode                           = \"global\"\n",
    "    al.match_score                    = 2\n",
    "    al.mismatch_score                 = -1.5\n",
    "    al.open_gap_score                 = -8\n",
    "    al.extend_gap_score               = -0.4\n",
    "    al.query_left_open_gap_score      = -8\n",
    "    al.query_left_extend_gap_score    = -0.4\n",
    "    al.query_right_open_gap_score     = -8\n",
    "    al.query_right_extend_gap_score   = -0.4\n",
    "    al.target_left_open_gap_score     = -8\n",
    "    al.target_left_extend_gap_score   = -0.4\n",
    "    al.target_right_open_gap_score    = -8\n",
    "    al.target_right_extend_gap_score  = -0.4\n",
    "    return al\n",
    "\n",
    "\n",
    "_aligner = _make_aligner()\n",
    "\n",
    "\n",
    "def parse_stoichiometry(stoich: str) -> list:\n",
    "    if pd.isna(stoich) or str(stoich).strip() == \"\":\n",
    "        return []\n",
    "    return [(ch.strip(), int(cnt)) for part in str(stoich).split(\";\")\n",
    "            for ch, cnt in [part.split(\":\")]]\n",
    "\n",
    "\n",
    "def parse_fasta(fasta_content: str) -> dict:\n",
    "    out, cur, parts = {}, None, []\n",
    "    for line in str(fasta_content).splitlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if line.startswith(\">\"):\n",
    "            if cur is not None:\n",
    "                out[cur] = \"\".join(parts)\n",
    "            cur = line[1:].split()[0]\n",
    "            parts = []\n",
    "        else:\n",
    "            parts.append(line.replace(\" \", \"\"))\n",
    "    if cur is not None:\n",
    "        out[cur] = \"\".join(parts)\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_chain_segments(row) -> list:\n",
    "    seq    = row[\"sequence\"]\n",
    "    stoich = row.get(\"stoichiometry\", \"\")\n",
    "    all_sq = row.get(\"all_sequences\", \"\")\n",
    "    if (pd.isna(stoich) or pd.isna(all_sq)\n",
    "            or str(stoich).strip() == \"\" or str(all_sq).strip() == \"\"):\n",
    "        return [(0, len(seq))]\n",
    "    try:\n",
    "        chain_dict = parse_fasta(all_sq)\n",
    "        order = parse_stoichiometry(stoich)\n",
    "        segs, pos = [], 0\n",
    "        for ch, cnt in order:\n",
    "            base = chain_dict.get(ch)\n",
    "            if base is None:\n",
    "                return [(0, len(seq))]\n",
    "            for _ in range(cnt):\n",
    "                segs.append((pos, pos + len(base)))\n",
    "                pos += len(base)\n",
    "        return segs if pos == len(seq) else [(0, len(seq))]\n",
    "    except Exception:\n",
    "        return [(0, len(seq))]\n",
    "\n",
    "\n",
    "def build_segments_map(df: pd.DataFrame) -> tuple:\n",
    "    seg_map, stoich_map = {}, {}\n",
    "    for _, r in df.iterrows():\n",
    "        tid               = r[\"target_id\"]\n",
    "        seg_map[tid]      = get_chain_segments(r)\n",
    "        raw_s             = r.get(\"stoichiometry\", \"\")\n",
    "        stoich_map[tid]   = \"\" if pd.isna(raw_s) else str(raw_s)\n",
    "    return seg_map, stoich_map\n",
    "\n",
    "\n",
    "def process_labels(labels_df: pd.DataFrame) -> dict:\n",
    "    coords = {}\n",
    "    prefixes = labels_df[\"ID\"].str.rsplit(\"_\", n=1).str[0]\n",
    "    for prefix, grp in labels_df.groupby(prefixes):\n",
    "        coords[prefix] = grp.sort_values(\"resid\")[[\"x_1\", \"y_1\", \"z_1\"]].values\n",
    "    return coords\n",
    "\n",
    "\n",
    "def _build_aligned_strings(query_seq, template_seq, alignment):\n",
    "    q_segs, t_segs = alignment.aligned\n",
    "    aq, at, qi, ti = [], [], 0, 0\n",
    "    for (qs, qe), (ts, te) in zip(q_segs, t_segs):\n",
    "        while qi < qs: aq.append(query_seq[qi]);    at.append(\"-\");              qi += 1\n",
    "        while ti < ts: aq.append(\"-\");              at.append(template_seq[ti]); ti += 1\n",
    "        for qp, tp in zip(range(qs, qe), range(ts, te)):\n",
    "            aq.append(query_seq[qp]); at.append(template_seq[tp])\n",
    "        qi, ti = qe, te\n",
    "    while qi < len(query_seq):    aq.append(query_seq[qi]);    at.append(\"-\");              qi += 1\n",
    "    while ti < len(template_seq): aq.append(\"-\");              at.append(template_seq[ti]); ti += 1\n",
    "    return \"\".join(aq), \"\".join(at)\n",
    "\n",
    "\n",
    "def find_similar_sequences_detailed(query_seq, train_seqs_df, train_coords_dict, top_n=30):\n",
    "    results = []\n",
    "    for _, row in train_seqs_df.iterrows():\n",
    "        tid, tseq = row[\"target_id\"], row[\"sequence\"]\n",
    "        if tid not in train_coords_dict:\n",
    "            continue\n",
    "        if abs(len(tseq) - len(query_seq)) / max(len(tseq), len(query_seq)) > 0.3:\n",
    "            continue\n",
    "        aln       = next(iter(_aligner.align(query_seq, tseq)))\n",
    "        norm_s    = aln.score / (2 * min(len(query_seq), len(tseq)))\n",
    "        identical = sum(\n",
    "            1 for (qs, qe), (ts, te) in zip(*aln.aligned)\n",
    "            for qp, tp in zip(range(qs, qe), range(ts, te))\n",
    "            if query_seq[qp] == tseq[tp]\n",
    "        )\n",
    "        pct_id = 100 * identical / len(query_seq)\n",
    "        aq, at = _build_aligned_strings(query_seq, tseq, aln)\n",
    "        results.append((tid, tseq, norm_s, train_coords_dict[tid], pct_id, aq, at))\n",
    "    results.sort(key=lambda x: x[2], reverse=True)\n",
    "    return results[:top_n]\n",
    "\n",
    "\n",
    "def adapt_template_to_query(query_seq, template_seq, template_coords) -> np.ndarray:\n",
    "    aln        = next(iter(_aligner.align(query_seq, template_seq)))\n",
    "    new_coords = np.full((len(query_seq), 3), np.nan)\n",
    "    for (qs, qe), (ts, te) in zip(*aln.aligned):\n",
    "        chunk = template_coords[ts:te]\n",
    "        if len(chunk) == (qe - qs):\n",
    "            new_coords[qs:qe] = chunk\n",
    "    for i in range(len(new_coords)):\n",
    "        if np.isnan(new_coords[i, 0]):\n",
    "            pv = next((j for j in range(i - 1, -1, -1) if not np.isnan(new_coords[j, 0])), -1)\n",
    "            nv = next((j for j in range(i + 1, len(new_coords)) if not np.isnan(new_coords[j, 0])), -1)\n",
    "            if pv >= 0 and nv >= 0:\n",
    "                w = (i - pv) / (nv - pv)\n",
    "                new_coords[i] = (1 - w) * new_coords[pv] + w * new_coords[nv]\n",
    "            elif pv >= 0:\n",
    "                new_coords[i] = new_coords[pv] + [3, 0, 0]\n",
    "            elif nv >= 0:\n",
    "                new_coords[i] = new_coords[nv] + [3, 0, 0]\n",
    "            else:\n",
    "                new_coords[i] = [i * 3, 0, 0]\n",
    "    return np.nan_to_num(new_coords)\n",
    "\n",
    "\n",
    "def adaptive_rna_constraints(coords, target_id, segments_map, confidence=1.0, passes=2) -> np.ndarray:\n",
    "    X        = coords.copy()\n",
    "    segments = segments_map.get(target_id, [(0, len(X))])\n",
    "    strength = max(0.75 * (1.0 - min(confidence, 0.90)), 0.02)\n",
    "    for _ in range(passes):\n",
    "        for s, e in segments:\n",
    "            C = X[s:e]; L = e - s\n",
    "            if L < 3:\n",
    "                continue\n",
    "            # bond i–i+1  ~5.95 Å\n",
    "            d    = C[1:] - C[:-1]; dist = np.linalg.norm(d, axis=1) + 1e-6\n",
    "            adj  = d * ((5.95 - dist) / dist)[:, None] * (0.22 * strength)\n",
    "            C[:-1] -= adj; C[1:] += adj\n",
    "            # soft i–i+2  ~10.2 Å\n",
    "            d2   = C[2:] - C[:-2]; d2n = np.linalg.norm(d2, axis=1) + 1e-6\n",
    "            adj2 = d2 * ((10.2 - d2n) / d2n)[:, None] * (0.10 * strength)\n",
    "            C[:-2] -= adj2; C[2:] += adj2\n",
    "            # Laplacian smoothing\n",
    "            C[1:-1] += (0.06 * strength) * (0.5 * (C[:-2] + C[2:]) - C[1:-1])\n",
    "            # self-avoidance\n",
    "            if L >= 25:\n",
    "                idx  = np.linspace(0, L - 1, min(L, 160)).astype(int) if L > 220 else np.arange(L)\n",
    "                P    = C[idx]; diff = P[:, None, :] - P[None, :, :]\n",
    "                dm   = np.linalg.norm(diff, axis=2) + 1e-6\n",
    "                sep  = np.abs(idx[:, None] - idx[None, :])\n",
    "                mask = (sep > 2) & (dm < 3.2)\n",
    "                if np.any(mask):\n",
    "                    vec = (diff * ((3.2 - dm) / dm)[:, :, None] * mask[:, :, None]).sum(axis=1)\n",
    "                    C[idx] += (0.015 * strength) * vec\n",
    "            X[s:e] = C\n",
    "    return X\n",
    "\n",
    "\n",
    "def _rotmat(axis, ang):\n",
    "    a = np.asarray(axis, float); a /= np.linalg.norm(a) + 1e-12\n",
    "    x, y, z = a; c, s = np.cos(ang), np.sin(ang); CC = 1 - c\n",
    "    return np.array([[c+x*x*CC, x*y*CC-z*s, x*z*CC+y*s],\n",
    "                     [y*x*CC+z*s, c+y*y*CC, y*z*CC-x*s],\n",
    "                     [z*x*CC-y*s, z*y*CC+x*s, c+z*z*CC]])\n",
    "\n",
    "\n",
    "def apply_hinge(coords, seg, rng, deg=22):\n",
    "    s, e = seg; L = e - s\n",
    "    if L < 30: return coords\n",
    "    pivot = s + int(rng.integers(10, L - 10))\n",
    "    R = _rotmat(rng.normal(size=3), np.deg2rad(float(rng.uniform(-deg, deg))))\n",
    "    X = coords.copy(); p0 = X[pivot].copy()\n",
    "    X[pivot+1:e] = (X[pivot+1:e] - p0) @ R.T + p0\n",
    "    return X\n",
    "\n",
    "\n",
    "def jitter_chains(coords, segs, rng, deg=12, trans=1.5):\n",
    "    X = coords.copy(); gc_ = X.mean(0, keepdims=True)\n",
    "    for s, e in segs:\n",
    "        R     = _rotmat(rng.normal(size=3), np.deg2rad(float(rng.uniform(-deg, deg))))\n",
    "        shift = rng.normal(size=3); shift = shift / (np.linalg.norm(shift) + 1e-12) * float(rng.uniform(0, trans))\n",
    "        c     = X[s:e].mean(0, keepdims=True)\n",
    "        X[s:e] = (X[s:e] - c) @ R.T + c + shift\n",
    "    X -= X.mean(0, keepdims=True) - gc_\n",
    "    return X\n",
    "\n",
    "\n",
    "def smooth_wiggle(coords, segs, rng, amp=0.8):\n",
    "    X = coords.copy()\n",
    "    for s, e in segs:\n",
    "        L = e - s\n",
    "        if L < 20: continue\n",
    "        ctrl = np.linspace(0, L - 1, 6); disp = rng.normal(0, amp, (6, 3)); t = np.arange(L)\n",
    "        X[s:e] += np.vstack([np.interp(t, ctrl, disp[:, k]) for k in range(3)]).T\n",
    "    return X\n",
    "\n",
    "\n",
    "def generate_rna_structure(sequence: str, seed=None) -> np.ndarray:\n",
    "    \"\"\"Idealized A-form RNA helix — last-resort de-novo fallback.\"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    n = len(sequence); coords = np.zeros((n, 3))\n",
    "    for i in range(n):\n",
    "        ang = i * 0.6\n",
    "        coords[i] = [10.0 * np.cos(ang), 10.0 * np.sin(ang), i * 2.5]\n",
    "    return coords\n",
    "\n",
    "\n",
    "# ─────────────── TBM Phase ───────────────────────────────────────────────────\n",
    "def tbm_phase(test_df, train_seqs_df, train_coords_dict, segments_map):\n",
    "    \"\"\"\n",
    "    Phase 1 — Template-Based Modeling.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    template_predictions : {target_id: [np.ndarray(seq_len, 3), ...]}\n",
    "        0 to N_SAMPLE predictions per target, from real templates.\n",
    "    protenix_queue : {target_id: (n_needed, full_sequence)}\n",
    "        Targets that still need more predictions.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PHASE 1: Template-Based Modeling\")\n",
    "    print(f\"  MIN_SIMILARITY = {MIN_SIMILARITY}  |  MIN_PCT_IDENTITY = {MIN_PERCENT_IDENTITY}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    template_predictions: dict = {}\n",
    "    protenix_queue:       dict = {}\n",
    "\n",
    "    for _, row in test_df.iterrows():\n",
    "        tid = row[\"target_id\"]\n",
    "        seq = row[\"sequence\"]\n",
    "        segs = segments_map.get(tid, [(0, len(seq))])\n",
    "\n",
    "        similar = find_similar_sequences_detailed(seq, train_seqs_df, train_coords_dict, top_n=30)\n",
    "        preds   = []\n",
    "        used    = set()\n",
    "\n",
    "        for i, (tmpl_id, tmpl_seq, sim, tmpl_coords, pct_id, _, _) in enumerate(similar):\n",
    "            if len(preds) >= N_SAMPLE:\n",
    "                break\n",
    "            if sim < MIN_SIMILARITY or pct_id < MIN_PERCENT_IDENTITY:\n",
    "                break           # list is sorted by sim, so no point continuing\n",
    "            if tmpl_id in used:\n",
    "                continue\n",
    "\n",
    "            rng     = np.random.default_rng((abs(hash(tid)) + i * 10007) % (2**32))\n",
    "            adapted = adapt_template_to_query(seq, tmpl_seq, tmpl_coords)\n",
    "\n",
    "            # Diversity transforms (same strategy as the 0-409 TBM notebook)\n",
    "            slot = len(preds)\n",
    "            if slot == 0:\n",
    "                X = adapted\n",
    "            elif slot == 1:\n",
    "                X = adapted + rng.normal(0, max(0.01, (0.40 - sim) * 0.06), adapted.shape)\n",
    "            elif slot == 2:\n",
    "                longest = max(segs, key=lambda se: se[1] - se[0])\n",
    "                X = apply_hinge(adapted, longest, rng)\n",
    "            elif slot == 3:\n",
    "                X = jitter_chains(adapted, segs, rng)\n",
    "            else:\n",
    "                X = smooth_wiggle(adapted, segs, rng)\n",
    "\n",
    "            refined = adaptive_rna_constraints(X, tid, segments_map, confidence=sim)\n",
    "            preds.append(refined)\n",
    "            used.add(tmpl_id)\n",
    "\n",
    "        template_predictions[tid] = preds\n",
    "        n_needed = N_SAMPLE - len(preds)\n",
    "        if n_needed > 0:\n",
    "            protenix_queue[tid] = (n_needed, seq)\n",
    "            print(f\"  {tid} ({len(seq)} nt): {len(preds)} TBM → need {n_needed} from Protenix\")\n",
    "        else:\n",
    "            print(f\"  {tid} ({len(seq)} nt): all {N_SAMPLE} from TBM ✓\")\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    n_full  = len(test_df) - len(protenix_queue)\n",
    "    print(f\"\\nPhase 1 done in {elapsed:.1f}s\")\n",
    "    print(f\"  Fully covered by TBM : {n_full}\")\n",
    "    print(f\"  Need Protenix        : {len(protenix_queue)}\")\n",
    "    return template_predictions, protenix_queue\n",
    "\n",
    "\n",
    "# ─────────────── Main ────────────────────────────────────────────────────────\n",
    "def main() -> None:\n",
    "    test_csv, output_csv, code_dir, root_dir = resolve_paths()\n",
    "\n",
    "    if not os.path.isdir(code_dir):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Missing PROTENIX_CODE_DIR: {code_dir}. \"\n",
    "            \"Set PROTENIX_CODE_DIR to the repo path.\"\n",
    "        )\n",
    "\n",
    "    os.environ[\"PROTENIX_ROOT_DIR\"] = root_dir\n",
    "    sys.path.append(code_dir)\n",
    "    ensure_required_files(root_dir)\n",
    "    seed_everything(SEED)\n",
    "\n",
    "    # ── Load test data ──────────────────────────────────────────────────────\n",
    "    test_df_full = pd.read_csv(test_csv)\n",
    "    test_df      = (test_df_full.head(LOCAL_N_SAMPLES) if not IS_KAGGLE\n",
    "                    else test_df_full).reset_index(drop=True)\n",
    "    print(f\"Test targets : {len(test_df)}\"\n",
    "          + (\" (LOCAL MODE)\" if not IS_KAGGLE else \"\"))\n",
    "\n",
    "    seq_by_id = dict(zip(test_df[\"target_id\"], test_df[\"sequence\"]))\n",
    "\n",
    "    # Truncated copy for Protenix (Protenix has token limits)\n",
    "    test_df_trunc = test_df.copy()\n",
    "    test_df_trunc[\"sequence\"] = test_df_trunc[\"sequence\"].str[:MAX_SEQ_LEN]\n",
    "\n",
    "    # ── Load training data for TBM ──────────────────────────────────────────\n",
    "    print(\"\\nLoading training data for TBM …\")\n",
    "    train_seqs   = pd.read_csv(DEFAULT_TRAIN_CSV)\n",
    "    val_seqs     = pd.read_csv(DEFAULT_VAL_CSV)\n",
    "    train_labels = pd.read_csv(DEFAULT_TRAIN_LBLS)\n",
    "    val_labels   = pd.read_csv(DEFAULT_VAL_LBLS)\n",
    "\n",
    "    combined_seqs   = pd.concat([train_seqs,   val_seqs],    ignore_index=True)\n",
    "    combined_labels = pd.concat([train_labels, val_labels],  ignore_index=True)\n",
    "    train_coords    = process_labels(combined_labels)\n",
    "    segments_map, _ = build_segments_map(test_df)\n",
    "\n",
    "    print(f\"Template pool: {len(combined_seqs)} sequences, {len(train_coords)} structures\")\n",
    "\n",
    "    # ─── PHASE 1: TBM ──────────────────────────────────────────────────────\n",
    "    template_preds, protenix_queue = tbm_phase(\n",
    "        test_df, combined_seqs, train_coords, segments_map\n",
    "    )\n",
    "\n",
    "    # ─── PHASE 2: Protenix (only for targets that need extra predictions) ──\n",
    "    protenix_preds: dict = {}   # target_id -> np.ndarray (n_needed, seq_len, 3)\n",
    "\n",
    "    if protenix_queue and USE_PROTENIX:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"PHASE 2: Protenix for {len(protenix_queue)} targets\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        work_dir = Path(\"/kaggle/working\")\n",
    "        work_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Build input JSON only for queued targets\n",
    "        queue_df = (test_df_trunc[test_df_trunc[\"target_id\"].isin(protenix_queue)]\n",
    "                    .reset_index(drop=True))\n",
    "        input_json_path = str(work_dir / \"protenix_queue_input.json\")\n",
    "        build_input_json(queue_df, input_json_path)\n",
    "\n",
    "        from protenix.data.inference.infer_dataloader import InferenceDataset\n",
    "        from runner.inference import (InferenceRunner,\n",
    "                                      update_gpu_compatible_configs,\n",
    "                                      update_inference_configs)\n",
    "\n",
    "        configs = build_configs(input_json_path, str(work_dir / \"outputs\"), MODEL_NAME)\n",
    "        configs = update_gpu_compatible_configs(configs)\n",
    "        runner  = InferenceRunner(configs)\n",
    "        dataset = InferenceDataset(configs)\n",
    "\n",
    "        for i in tqdm(range(len(dataset)), desc=\"Protenix\"):\n",
    "            data, atom_array, error_message = dataset[i]\n",
    "            target_id = data.get(\"sample_name\", f\"sample_{i}\")\n",
    "\n",
    "            if target_id not in protenix_queue:\n",
    "                continue\n",
    "\n",
    "            n_needed, full_seq = protenix_queue[target_id]\n",
    "\n",
    "            if error_message:\n",
    "                print(f\"  {target_id}: data error — {error_message}\")\n",
    "                protenix_preds[target_id] = None\n",
    "                del data, atom_array, error_message\n",
    "                gc.collect(); torch.cuda.empty_cache(); gc.collect()\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                new_cfg = update_inference_configs(configs, data[\"N_token\"].item())\n",
    "                # Only generate as many samples as we actually need to fill the slots\n",
    "                new_cfg.sample_diffusion.N_sample = n_needed\n",
    "                runner.update_model_configs(new_cfg)\n",
    "\n",
    "                prediction = runner.predict(data)\n",
    "                raw_coords = prediction[\"coordinate\"] # Shape: [N_sample, all_atoms, 3]\n",
    "\n",
    "                # -----------------------------------------------------------\n",
    "                # DEBUG PRINT START\n",
    "                # -----------------------------------------------------------\n",
    "                print(f\"\\n[DEBUG] {target_id} | n_needed: {n_needed} | SeqLen: {len(full_seq)}\")\n",
    "                print(f\"[DEBUG] raw_coords shape: {raw_coords.shape}\")\n",
    "                \n",
    "                feat = data[\"input_feature_dict\"]\n",
    "                \n",
    "                # Check potential masks\n",
    "                mask_candidates = {}\n",
    "                if \"centre_atom_mask\" in feat:\n",
    "                    m = feat[\"centre_atom_mask\"]\n",
    "                    mask_candidates['centre_atom_mask'] = (m.sum().item(), m.shape)\n",
    "                \n",
    "                if \"atom_to_tokatom_idx\" in feat:\n",
    "                    idx_11 = (feat[\"atom_to_tokatom_idx\"] == 11).sum().item()\n",
    "                    idx_12 = (feat[\"atom_to_tokatom_idx\"] == 12).sum().item()\n",
    "                    mask_candidates['idx_11'] = idx_11\n",
    "                    mask_candidates['idx_12'] = idx_12\n",
    "                \n",
    "                print(f\"[DEBUG] Mask candidates counts: {mask_candidates}\")\n",
    "                # -----------------------------------------------------------\n",
    "                # DEBUG PRINT END\n",
    "                # -----------------------------------------------------------\n",
    "\n",
    "                # ─────────────────────────────────────────────────────────────\n",
    "                # DEBUG / FIX: Explicit C1' masking logic\n",
    "                # ─────────────────────────────────────────────────────────────\n",
    "                # Try to use 'centre_atom_mask' from features if possible\n",
    "                if \"centre_atom_mask\" in feat:\n",
    "                    mask = (feat[\"centre_atom_mask\"] == 1).to(raw_coords.device)\n",
    "                elif \"atom_to_tokatom_idx\" in feat:\n",
    "                    # Heuristic: pick the one closest to sequence length\n",
    "                    m11 = (feat[\"atom_to_tokatom_idx\"] == 11).to(raw_coords.device)\n",
    "                    m12 = (feat[\"atom_to_tokatom_idx\"] == 12).to(raw_coords.device)\n",
    "                    \n",
    "                    c11, c12 = m11.sum(), m12.sum()\n",
    "                    target_len = len(full_seq) # closer to N_token usually\n",
    "                    \n",
    "                    if abs(c11 - target_len) < abs(c12 - target_len):\n",
    "                         mask = m11\n",
    "                         print(f\"[DEBUG] Selected idx 11 mask (count={c11})\")\n",
    "                    else:\n",
    "                         mask = m12\n",
    "                         print(f\"[DEBUG] Selected idx 12 mask (count={c12})\")\n",
    "                else:\n",
    "                    # Should not happen\n",
    "                    mask = torch.zeros(raw_coords.shape[1], dtype=torch.bool, device=raw_coords.device)\n",
    "                \n",
    "                # Extract\n",
    "                coords = raw_coords[:, mask, :].detach().cpu().numpy()\n",
    "                print(f\"[DEBUG] Extracted coords shape: {coords.shape}\")\n",
    "\n",
    "                # If we get duplicate coordinates (collapsed), this is bad.\n",
    "                # Check for duplications in first sample\n",
    "                if coords.shape[1] > 1:\n",
    "                     diffs = np.linalg.norm(coords[0, 1:] - coords[0, :-1], axis=-1)\n",
    "                     if np.all(diffs < 1e-4):\n",
    "                         print(f\"  WARNING: {target_id} has identical coordinates for all residues! (Model collapse?)\")\n",
    "                \n",
    "                # Pad/trim to full (un-truncated) sequence length\n",
    "                if coords.shape[1] != len(full_seq):\n",
    "                    # Check for broadcast issue or model collapse\n",
    "                    if coords.shape[1] == 1 and len(full_seq) > 1:\n",
    "                        # Model outputted only 1 residue/atom but we need many?\n",
    "                        # Broadcast the single coord to all positions just in case (though highly suspicious)\n",
    "                        # Or perhaps mask was wrong and selected only 1 atom.\n",
    "                        # Do NOT broadcast, fill with zeros to be safe.\n",
    "                        print(f\"[DEBUG] WARNING: {target_id}: mask selected only 1 atom, but sequence is {len(full_seq)}\")\n",
    "                        # padded = np.zeros(...) -> kept as zeros\n",
    "                    else:\n",
    "                        padded  = np.zeros((coords.shape[0], len(full_seq), 3), dtype=np.float32)\n",
    "                        min_len = min(coords.shape[1], len(full_seq))\n",
    "                        if min_len > 0:\n",
    "                            padded[:, :min_len, :] = coords[:, :min_len, :]\n",
    "                        coords = padded\n",
    "\n",
    "                # Final check for identical coordinates (indicative of model failure)\n",
    "                if coords.shape[1] > 1:\n",
    "                     diffs = np.linalg.norm(coords[0, 1:] - coords[0, :-1], axis=-1)\n",
    "                     if np.all(diffs < 1e-4):\n",
    "                         print(f\"  WARNING: {target_id}: Identical coordinates detected! Resetting to zeros.\")\n",
    "                         coords = np.zeros_like(coords)\n",
    "\n",
    "                protenix_preds[target_id] = coords\n",
    "                print(f\"  {target_id}: {coords.shape[0]} Protenix predictions generated\")\n",
    "\n",
    "            except Exception as exc:\n",
    "                print(f\"  {target_id}: Protenix FAILED — {exc}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                protenix_preds[target_id] = None\n",
    "\n",
    "            finally:\n",
    "                del prediction, raw_coords, mask, data, atom_array\n",
    "                gc.collect(); torch.cuda.empty_cache(); gc.collect()\n",
    "# ...existing code...\n",
    "\n",
    "    elif protenix_queue and not USE_PROTENIX:\n",
    "        print(f\"\\nPHASE 2 skipped (USE_PROTENIX=False). \"\n",
    "              f\"De-novo fallback will cover {len(protenix_queue)} targets.\")\n",
    "\n",
    "    # ─── PHASE 3: Combine everything ───────────────────────────────────────\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"PHASE 3: Combine TBM + Protenix + de-novo fallback\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    all_rows = []\n",
    "\n",
    "    for _, row in test_df.iterrows():\n",
    "        tid = row[\"target_id\"]\n",
    "        seq = row[\"sequence\"]\n",
    "\n",
    "        combined: list = list(template_preds.get(tid, []))  # TBM predictions\n",
    "\n",
    "        # Append Protenix predictions to fill remaining slots\n",
    "        ptx = protenix_preds.get(tid)\n",
    "        if ptx is not None and ptx.ndim == 3:\n",
    "            for j in range(ptx.shape[0]):\n",
    "                if len(combined) >= N_SAMPLE:\n",
    "                    break\n",
    "                combined.append(ptx[j])  # (seq_len, 3)\n",
    "\n",
    "        # De-novo fallback for any still-empty slots\n",
    "        n_denovo = 0\n",
    "        while len(combined) < N_SAMPLE:\n",
    "            seed_val = hash(tid) % 10000 + len(combined) * 1000\n",
    "            dn       = generate_rna_structure(seq, seed=seed_val)\n",
    "            combined.append(adaptive_rna_constraints(dn, tid, segments_map, confidence=0.2))\n",
    "            n_denovo += 1\n",
    "\n",
    "        if n_denovo:\n",
    "            print(f\"  {tid}: {n_denovo} slot(s) filled with de-novo fallback\")\n",
    "\n",
    "        # Stack to (N_SAMPLE, seq_len, 3) and write rows\n",
    "        stacked = np.stack(combined[:N_SAMPLE], axis=0)\n",
    "        all_rows.extend(coords_to_rows(tid, seq, stacked))\n",
    "\n",
    "    # ── Save ───────────────────────────────────────────────────────────────\n",
    "    sub = pd.DataFrame(all_rows)\n",
    "    cols = [\"ID\", \"resname\", \"resid\"] + [\n",
    "        f\"{c}_{i}\" for i in range(1, N_SAMPLE + 1) for c in [\"x\", \"y\", \"z\"]\n",
    "    ]\n",
    "    coord_cols = [c for c in cols if c.startswith((\"x_\", \"y_\", \"z_\"))]\n",
    "    sub[coord_cols] = sub[coord_cols].clip(-999.999, 9999.999)\n",
    "    sub[cols].to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\"\\n✓ Saved submission to {output_csv}  ({len(sub):,} rows)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9de2b750",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T21:41:35.046685Z",
     "iopub.status.busy": "2026-02-20T21:41:35.046289Z",
     "iopub.status.idle": "2026-02-20T21:41:35.069256Z",
     "shell.execute_reply": "2026-02-20T21:41:35.068058Z"
    },
    "papermill": {
     "duration": 0.030205,
     "end_time": "2026-02-20T21:41:35.071518",
     "exception": false,
     "start_time": "2026-02-20T21:41:35.041313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created symlink: /kaggle/working/protenix_root/mmcif -> /kaggle/input/stanford-rna-3d-folding-2/PDB_RNA\n",
      "Created symlink: /kaggle/working/protenix_root/common -> /kaggle/input/datasets/qiweiyin/protenix-v1-adjusted/Protenix-v1-adjust-v2/Protenix-v1-adjust-v2/Protenix-v1/common\n",
      "Created symlink: /kaggle/working/protenix_root/checkpoint -> /kaggle/input/datasets/qiweiyin/protenix-v1-adjusted/Protenix-v1-adjust-v2/Protenix-v1-adjust-v2/Protenix-v1/checkpoint\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# FIX: Setup writable directory for Templates\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# The competition PDB_RNA folder contains .cif files we can use as templates.\n",
    "# We create a writable 'root' directory and symlink the read-only data into it.\n",
    "\n",
    "# 1. New writable root for Protenix data\n",
    "NEW_ROOT = \"/kaggle/working/protenix_root\"\n",
    "os.makedirs(NEW_ROOT, exist_ok=True)\n",
    "os.environ[\"PROTENIX_ROOT_DIR\"] = NEW_ROOT\n",
    "\n",
    "# 2. Symlink the competition's PDB_RNA to 'mmcif' (where Protenix looks for templates)\n",
    "#    Source: /kaggle/input/stanford-rna-3d-folding-2/PDB_RNA ({pdb_id}.cif)\n",
    "#    Target: /kaggle/working/protenix_root/mmcif\n",
    "src_pdb = \"/kaggle/input/stanford-rna-3d-folding-2/PDB_RNA\"\n",
    "dst_pdb = f\"{NEW_ROOT}/mmcif\"\n",
    "\n",
    "if not os.path.exists(dst_pdb):\n",
    "    try:\n",
    "        os.symlink(src_pdb, dst_pdb)\n",
    "        print(f\"Created symlink: {dst_pdb} -> {src_pdb}\")\n",
    "    except OSError:\n",
    "        # Fallback if symlink fails (copying is slow but safe)\n",
    "        import shutil\n",
    "        print(\"Symlink failed, copying PDB_RNA folder (this may take a minute)...\")\n",
    "        shutil.copytree(src_pdb, dst_pdb)\n",
    "\n",
    "# 3. Symlink 'common' and 'checkpoint' from the original dataset\n",
    "#    The original dataset path (adjust if your dataset path is different):\n",
    "ORIGINAL_DATA_DIR = \"/kaggle/input/datasets/qiweiyin/protenix-v1-adjusted/Protenix-v1-adjust-v2/Protenix-v1-adjust-v2/Protenix-v1\"\n",
    "\n",
    "for folder in [\"common\", \"checkpoint\"]:\n",
    "    src = f\"{ORIGINAL_DATA_DIR}/{folder}\"\n",
    "    dst = f\"{NEW_ROOT}/{folder}\"\n",
    "    if not os.path.exists(dst):\n",
    "        if os.path.exists(src):\n",
    "            os.symlink(src, dst)\n",
    "            print(f\"Created symlink: {dst} -> {src}\")\n",
    "        else:\n",
    "            print(f\"WARNING: Could not find original {folder} at {src}\")\n",
    "\n",
    "# 4. Update the path resolution function to use our new root\n",
    "def resolve_paths():\n",
    "    test_csv   = os.environ.get(\"TEST_CSV\",           DEFAULT_TEST_CSV)\n",
    "    output_csv = os.environ.get(\"SUBMISSION_CSV\",     DEFAULT_OUTPUT)\n",
    "    code_dir   = os.environ.get(\"PROTENIX_CODE_DIR\",  DEFAULT_CODE_DIR)\n",
    "    # FORCE the root dir to be our new writable dir\n",
    "    root_dir   = NEW_ROOT \n",
    "    return test_csv, output_csv, code_dir, root_dir # Returns new root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "187bd191",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T21:41:35.080686Z",
     "iopub.status.busy": "2026-02-20T21:41:35.080272Z",
     "iopub.status.idle": "2026-02-20T21:41:35.085458Z",
     "shell.execute_reply": "2026-02-20T21:41:35.084184Z"
    },
    "papermill": {
     "duration": 0.012081,
     "end_time": "2026-02-20T21:41:35.087357",
     "exception": false,
     "start_time": "2026-02-20T21:41:35.075276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# /kaggle/input/datasets/qiweiyin/protenix-v1-adjusted/Protenix-v1-adjust-v2/Protenix-v1-adjust-v2/Protenix-v1/checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bfb1e85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T21:41:35.096703Z",
     "iopub.status.busy": "2026-02-20T21:41:35.096014Z",
     "iopub.status.idle": "2026-02-20T21:41:35.106569Z",
     "shell.execute_reply": "2026-02-20T21:41:35.105423Z"
    },
    "papermill": {
     "duration": 0.017651,
     "end_time": "2026-02-20T21:41:35.108649",
     "exception": false,
     "start_time": "2026-02-20T21:41:35.090998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symlinked /kaggle/input/stanford-rna-3d-folding-2/PDB_RNA -> /kaggle/working/protenix_data/mmcif\n"
     ]
    }
   ],
   "source": [
    "# ─── FIX for USE_TEMPLATE ────────────────────────────────────────────────────\n",
    "# Protenix expects a specific folder structure for templates.\n",
    "# We create a writable root in /kaggle/working and symlink the necessary data.\n",
    "\n",
    "# 1. Define writable root\n",
    "WRITABLE_ROOT = \"/kaggle/working/protenix_data\"\n",
    "os.makedirs(WRITABLE_ROOT, exist_ok=True)\n",
    "os.environ[\"PROTENIX_ROOT_DIR\"] = WRITABLE_ROOT\n",
    "\n",
    "# 2. Link the competition's PDB_RNA folder to 'mmcif' (where Protenix looks)\n",
    "#    Note: Protenix expects $ROOT/mmcif to contain the .cif files\n",
    "mmcif_target = f\"{WRITABLE_ROOT}/mmcif\"\n",
    "if not os.path.exists(mmcif_target):\n",
    "    # Symlink the directory directly if possible, or create folder and link files\n",
    "    # The competition data is at: /kaggle/input/stanford-rna-3d-folding-2/PDB_RNA\n",
    "    source_pdb = \"/kaggle/input/stanford-rna-3d-folding-2/PDB_RNA\"\n",
    "    \n",
    "    # We create a symlink to the folder. \n",
    "    # Valid validation: Protenix checks if os.path.exists(template_mmcif_dir)\n",
    "    os.symlink(source_pdb, mmcif_target)\n",
    "    print(f\"Symlinked {source_pdb} -> {mmcif_target}\")\n",
    "\n",
    "# 3. Link other static assets from the original read-only code dataset\n",
    "#    The original code expected data in DEFAULT_CODE_DIR/../.. or similar.\n",
    "#    We need 'common' folder (components.cif etc) and 'checkpoint'.\n",
    "\n",
    "# Original read-only data assets path\n",
    "# Based on your previous configs, the assets seem to be here:\n",
    "READONLY_ASSETS = \"/kaggle/input/datasets/qiweiyin/protenix-v1-adjusted/Protenix-v1-adjust-v2/Protenix-v1-adjust-v2\"\n",
    "\n",
    "for folder in [\"common\", \"checkpoint\"]:\n",
    "    src = f\"{READONLY_ASSETS}/{folder}\"\n",
    "    dst = f\"{WRITABLE_ROOT}/{folder}\"\n",
    "    if not os.path.exists(dst) and os.path.exists(src):\n",
    "        os.symlink(src, dst)\n",
    "        print(f\"Symlinked {src} -> {dst}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475a94ed",
   "metadata": {
    "papermill": {
     "duration": 0.003556,
     "end_time": "2026-02-20T21:41:35.116040",
     "exception": false,
     "start_time": "2026-02-20T21:41:35.112484",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66427249",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T21:41:35.124845Z",
     "iopub.status.busy": "2026-02-20T21:41:35.124506Z",
     "iopub.status.idle": "2026-02-20T21:52:40.763097Z",
     "shell.execute_reply": "2026-02-20T21:52:40.761520Z"
    },
    "papermill": {
     "duration": 665.646638,
     "end_time": "2026-02-20T21:52:40.766279",
     "exception": false,
     "start_time": "2026-02-20T21:41:35.119641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test targets : 2 (LOCAL MODE)\n",
      "\n",
      "Loading training data for TBM …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34/3194524112.py:542: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_labels = pd.read_csv(DEFAULT_TRAIN_LBLS)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template pool: 5744 sequences, 5744 structures\n",
      "\n",
      "============================================================\n",
      "PHASE 1: Template-Based Modeling\n",
      "  MIN_SIMILARITY = 0.0  |  MIN_PCT_IDENTITY = 50.0\n",
      "============================================================\n",
      "  8ZNQ (30 nt): 2 TBM → need 5 from Protenix\n",
      "  9IWF (69 nt): all 7 from TBM ✓\n",
      "\n",
      "Phase 1 done in 1.7s\n",
      "  Fully covered by TBM : 1\n",
      "  Need Protenix        : 1\n",
      "\n",
      "============================================================\n",
      "PHASE 2: Protenix for 1 targets\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-20 21:42:23,010 [/kaggle/input/datasets/qiweiyin/protenix-v1-adjusted/Protenix-v1-adjust-v2/Protenix-v1-adjust-v2/Protenix-v1/runner/inference.py:246] INFO runner.inference: Distributed environment: world size: 1, global rank: 0, local rank: 0\n",
      "2026-02-20 21:42:23,012 [/kaggle/input/datasets/qiweiyin/protenix-v1-adjusted/Protenix-v1-adjust-v2/Protenix-v1-adjust-v2/Protenix-v1/runner/inference.py:127] INFO root: Finished environment initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train scheduler 16.0\n",
      "inference scheduler 16.0\n",
      "Diffusion Module has 16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-20 21:43:56,949 [/kaggle/input/datasets/qiweiyin/protenix-v1-adjusted/Protenix-v1-adjust-v2/Protenix-v1-adjust-v2/Protenix-v1/runner/inference.py:246] INFO runner.inference: Loading from /kaggle/working/protenix_root/checkpoint/protenix_base_20250630_v1.0.0.pt, strict: True\n",
      "2026-02-20 21:44:08,326 [/kaggle/input/datasets/qiweiyin/protenix-v1-adjusted/Protenix-v1-adjust-v2/Protenix-v1-adjust-v2/Protenix-v1/runner/inference.py:246] INFO runner.inference: Sampled key: module.input_embedder.atom_attention_encoder.linear_no_bias_ref_pos.weight\n",
      "2026-02-20 21:44:08,615 [/kaggle/input/datasets/qiweiyin/protenix-v1-adjusted/Protenix-v1-adjust-v2/Protenix-v1-adjust-v2/Protenix-v1/runner/inference.py:246] INFO runner.inference: Finish loading checkpoint.\n",
      "2026-02-20 21:44:08,631 [/kaggle/input/datasets/qiweiyin/protenix-v1-adjusted/Protenix-v1-adjust-v2/Protenix-v1-adjust-v2/Protenix-v1/runner/inference.py:246] INFO runner.inference: Model parameters: 368.48M\n",
      "Protenix:   0%|          | 0/1 [00:00<?, ?it/s]2026-02-20 21:44:08,711 [/kaggle/input/datasets/qiweiyin/protenix-v1-adjusted/Protenix-v1-adjust-v2/Protenix-v1-adjust-v2/Protenix-v1/protenix/data/inference/infer_dataloader.py:281] INFO protenix.data.inference.infer_dataloader: Featurizing 8ZNQ...\n",
      "2026-02-20 21:44:21,693 [/kaggle/input/datasets/qiweiyin/protenix-v1-adjusted/Protenix-v1-adjust-v2/Protenix-v1-adjust-v2/Protenix-v1/protenix/data/constraint/constraint_featurizer.py:392] INFO protenix.data.constraint.constraint_featurizer: Loaded constraint feature: #atom contact:0 #contact:0 #pocket:0\n",
      "2026-02-20 21:44:21,745 [/kaggle/input/datasets/qiweiyin/protenix-v1-adjusted/Protenix-v1-adjust-v2/Protenix-v1-adjust-v2/Protenix-v1/protenix/data/template/template_featurizer.py:668] INFO protenix.data.template.template_featurizer: Calling InferenceTemplateFeaturizer.make_template_feature\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DEBUG] 8ZNQ | n_needed: 5 | SeqLen: 30\n",
      "[DEBUG] raw_coords shape: torch.Size([5, 639, 3])\n",
      "[DEBUG] Mask candidates counts: {'idx_11': 30, 'idx_12': 30}\n",
      "[DEBUG] Selected idx 12 mask (count=30)\n",
      "[DEBUG] Extracted coords shape: (5, 30, 3)\n",
      "  8ZNQ: 5 Protenix predictions generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Protenix: 100%|██████████| 1/1 [08:30<00:00, 510.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PHASE 3: Combine TBM + Protenix + de-novo fallback\n",
      "============================================================\n",
      "\n",
      "✓ Saved submission to /kaggle/working/submission.csv  (99 rows)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e391263",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T21:52:40.779059Z",
     "iopub.status.busy": "2026-02-20T21:52:40.778678Z",
     "iopub.status.idle": "2026-02-20T21:52:40.805783Z",
     "shell.execute_reply": "2026-02-20T21:52:40.804473Z"
    },
    "papermill": {
     "duration": 0.036547,
     "end_time": "2026-02-20T21:52:40.808024",
     "exception": false,
     "start_time": "2026-02-20T21:52:40.771477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ID resname  resid        x_1        y_1        z_1         x_2  \\\n",
      "0    8ZNQ_1       A      1  -2.054822 -15.061297  20.749760  144.908296   \n",
      "1    8ZNQ_2       C      2  -1.972113 -15.074823  15.340872  141.458825   \n",
      "2    8ZNQ_3       C      3  -3.343626 -13.506629  10.460660  138.505098   \n",
      "3    8ZNQ_4       G      4  -5.447613 -11.030060   6.595905  142.549535   \n",
      "4    8ZNQ_5       U      5  -6.348738  -6.232960   4.532983  143.616143   \n",
      "5    8ZNQ_6       G      6  -6.899965  -0.864735   3.218299  142.451711   \n",
      "6    8ZNQ_7       A      7  -4.529249   4.096138  -0.853225  139.493944   \n",
      "7    8ZNQ_8       C      8  -5.376084   9.736445   3.313003  137.186288   \n",
      "8    8ZNQ_9       G      9  -0.407463  10.609613   0.132231  135.694573   \n",
      "9   8ZNQ_10       G     10   3.911901   9.960426  -2.059070  134.468601   \n",
      "10  8ZNQ_11       G     11   7.975030   7.507895  -4.511413  133.182546   \n",
      "11  8ZNQ_12       C     12  10.176958   4.499823  -8.990600  132.500442   \n",
      "12  8ZNQ_13       C     13   9.849770   3.078332 -14.376044  131.509745   \n",
      "13  8ZNQ_14       U     14   7.522451   4.510508 -18.526894  129.679328   \n",
      "14  8ZNQ_15       U     15   8.257280  11.816229 -21.767715  127.996175   \n",
      "15  8ZNQ_16       U     16   0.184391  11.221678 -20.936141  130.240495   \n",
      "16  8ZNQ_17       U     17  -3.568972   8.219745 -15.732052  138.944879   \n",
      "17  8ZNQ_18       G     18  -0.880178   2.184813 -16.326718  144.323467   \n",
      "18  8ZNQ_19       G     19   2.350265  -0.735549 -13.495678  147.481487   \n",
      "19  8ZNQ_20       C     20   5.652282  -1.848133  -9.223890  149.446874   \n",
      "\n",
      "           y_2         z_2       x_3  ...       z_4        x_5        y_5  \\\n",
      "0   185.033180  167.727997  4.651348  ...  9.259408 -13.171015 -12.059234   \n",
      "1   184.782519  167.037770  0.236361  ...  9.244261  -8.231470 -13.765427   \n",
      "2   185.187615  168.056115 -4.808329  ...  6.479488  -3.878182 -13.552619   \n",
      "3   187.086505  172.805379 -7.914389  ...  1.883806  -1.360992 -10.710673   \n",
      "4   190.501574  176.419050 -7.814590  ... -2.156745  -0.835040  -6.008846   \n",
      "5   194.723799  179.351689 -5.770272  ... -5.474503  -1.545336  -0.598527   \n",
      "6   198.803021  180.980166 -1.907651  ... -7.047367  -2.436491   4.392914   \n",
      "7   199.580858  179.828335  2.895535  ... -6.715878  -3.918626   8.369567   \n",
      "8   199.469906  178.707823  6.298651  ... -3.115328  -4.111028  10.394694   \n",
      "9   199.597459  178.097764  7.776717  ...  1.802523  -1.852300   9.524341   \n",
      "10  200.168064  177.946231  7.320294  ...  6.574836   2.167389   7.927931   \n",
      "11  200.071364  176.721689  4.627087  ...  8.540850   7.401487   6.986876   \n",
      "12  199.738142  175.815086  0.646957  ...  7.625420  12.586851   7.060633   \n",
      "13  200.591882  176.223592 -2.621101  ...  4.418100  15.679508   9.556086   \n",
      "14  200.394571  174.929027 -2.561636  ... -0.420391  23.152916  12.946545   \n",
      "15  200.561114  172.384365 -5.299202  ... -5.284850  15.624716  12.851587   \n",
      "16  200.116570  173.180535 -9.726088  ... -7.932605  16.267422  10.163761   \n",
      "17  197.177076  171.774805 -7.640596  ... -1.657137  14.767961   6.929740   \n",
      "18  192.561941  169.441902 -5.749255  ...  3.519580  13.213003   3.093414   \n",
      "19  189.159291  164.290331 -2.466247  ...  6.970514   9.263741   1.002807   \n",
      "\n",
      "         z_5        x_6        y_6        z_6        x_7        y_7        z_7  \n",
      "0   3.277842 -11.707466   1.264317  12.487450   1.569918 -10.398350 -16.112787  \n",
      "1   1.581680  -6.975602   3.608469  12.112152  -0.188718  -5.246742 -16.293812  \n",
      "2  -2.128400  -1.320642   3.560864  11.458891  -3.666433  -1.096044 -14.462200  \n",
      "3  -6.152836   3.217193   0.820583   9.549740  -6.935758   0.945017 -10.410770  \n",
      "4  -8.523355   4.950929  -2.930480   6.364656  -8.060369   0.608100  -5.338024  \n",
      "5  -9.583219   4.567146  -6.843329   2.684121  -7.721087  -1.208907  -0.231271  \n",
      "6  -8.590160   2.769016  -9.775840  -1.242758  -5.427840  -3.337736   3.819325  \n",
      "7  -5.200628  -0.529847 -11.364695  -5.365265  -1.898360  -6.301428   7.372296  \n",
      "8  -0.274056  -3.499799  -9.228561  -8.834925   3.085319  -6.296185   8.500869  \n",
      "9   4.681769  -5.441036  -4.481469 -10.854950   8.108796  -3.840590   7.345506  \n",
      "10  8.553320  -5.285037   1.180983 -12.374856  11.589668   0.420263   6.103526  \n",
      "11  9.626226  -2.413344   5.439331 -14.044888  12.302036   5.727852   6.098432  \n",
      "12  7.857416   2.323939   7.614986 -15.838085  10.647013  10.818978   7.236670  \n",
      "13  4.393852   6.915569   6.873718 -18.303492   7.664611  13.816044  10.348125  \n",
      "14  1.514069  10.592854   3.719103 -23.732964   5.899579  15.508556  15.404989  \n",
      "15 -0.970573  10.103844   1.618234 -18.816311   0.752660  12.947479  17.303907  \n",
      "16 -9.209660  15.899964  -3.332642 -15.930749  -0.859108  14.284071  12.627942  \n",
      "17 -2.481261  11.069437   2.288498 -13.476110   0.349176  12.278893   8.711283  \n",
      "18  1.812713   7.434155   5.892354 -10.749851   3.624825  11.495016   4.370351  \n",
      "19  5.058618   2.652377   6.701734  -8.388166   6.969656   8.500167   1.131636  \n",
      "\n",
      "[20 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "#read submission.csv\n",
    "submission_path = \"/kaggle/working/submission.csv\"\n",
    "submission_df = pd.read_csv(submission_path)\n",
    "print(submission_df.head(20))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15231210,
     "sourceId": 118765,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 15736806,
     "datasetId": 9502242,
     "sourceId": 14874339,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 11219268,
     "datasetId": 6742586,
     "sourceId": 10855324,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 678.736491,
   "end_time": "2026-02-20T21:52:44.316903",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-20T21:41:25.580412",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
